{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز اول پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۸ فروردین <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=6>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    پروژه‌ی درس متشکل از ۳ فاز است.\n",
    "    دادگان مورد استفاده در پروژه، مقالات علمی استخراج شده از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> هستند. مقالات به دو دسته‌ی \n",
    "    «هوش مصنوعی , بیوانفورماتیک» و «سخت‌افزار و سیستم» \n",
    "    تقسیم شده‌اند. تخصصی بودن حوزه‌ی بازیابی می‌تواند به بهبود کیفیت آن کمک نماید.\n",
    "    <b><u>\n",
    "    پیش از هر چیز ابتدا یکی از این دو دسته را برای کار انتخاب نمایید.\n",
    "    </u></b>\n",
    "     امیدواریم تا انتهای پروژه بتوانید برای یکی از این دسته‌ها یک سیستم جست‌و‌جوی مقالات بسیار خوب پیاده‌سازی کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>فاز اول</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    هدف از فاز اول پروژه، طراحی و پیاده سازی سیستم بازیابی اطلاعات برای مجموعه دادگان ارائه شده است. در این فاز شناسه، چکیده و عنوان مقالات در اختیارتان قرار گرفته است.\n",
    "    در ابتدای هر بخش توضیح کوتاهی درباره آن بخش آورده شده است. در تمامی کد نیاز است که قسمت‌های TODO را کامل کنید. ملاک نمره دهی در این فاز صحت عملکرد توابع سیستم است. بنابراین از اجرا شدن کد خود اطمینان حاصل کنید و هر جا نیاز به توضیح بود می‌توانید به صورت کامنت یا در ادامه کد توضیح مربوطه خود را بنویسید (ارائه توضیح ضروری نیست و تنها در صورتی که شما احساس نیاز کردید، می‌توانید توضیح کوتاهی ارائه کنید. بنابراین ارائه توضیح نمره نخواهد داشت.) در زمان آپلود فراموش نکنید هر فایلی که نیاز است را آپلود کنید. نمره ی کامل این فاز ۱۰۰ بوده و مابقی امتیازیست.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیش‌پردازش و آماده‌سازی داده‌ها (۶ + ۲ + ۱ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    در بخش اول ابتدا داده‌ها را از فایل خوانده و سپس با استفاده از کتابخانه‌های آماده‌ به پیش پردازش آنها بپردازید. در این قسمت نحوه پیاده سازی به شما برمی‌گردد. کتابخانه‌هایی که می‌توانید از آنها استفاده کنید <a href=\"https://spacy.io/\">SpaCy</a>  و <a href=\"https://www.nltk.org/\">NLTK</a>  است. \n",
    "    در این قسمت تابع clean_data() را پیاده سازی کنید. عملکرد تابع به این صورت است که یک متن به عنوان ورودی گرفته و توکن‌های ولید آن را به صورت یک لیست که عملیات lemmatization, stemming و case folding اجرا شده است خروجی می‌دهد. متن ورودی شامل عنوان مقاله یا چکیده آنهاست. \n",
    "    دقت کنید علائم نگارشی باید از متون حذف شده باشند. در قسمت بعد کلمات اضافه بی تاثیر را یافته و مطابق توضیحات عمل کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'abstract', 'is', 'a', 'summari', 'of', 'the', 'main', 'articl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 6 points\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_data(text : str):\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization, and punctuations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The title or abstract of an article\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: tokenize, case_folding, stem, lemmatize, punctuations    \n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    text = [word.lower() for word in text]\n",
    "    \n",
    "    text = [word for word in text if word not in string.punctuation and not word.isdigit()]\n",
    "    \n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "        \n",
    "    return text\n",
    "    \n",
    "\n",
    "clean_data(\"An abstract is a summary of the main article.\") # return [\"an\", \"abstract\", \"is\", \"a\", \"summary\", \"of\", \"the\", \"main\", \"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در مرحله بعد تابع find_stop_words() را پیاده سازی کنید که کارکرد آن پیدا کردن توکن‌های اضافه است. \n",
    "توجه کنید که برای حذف stop wordها باید به گونه ای عمل کنید که ابتدا توکن‌های با بیشترین تکرار را  پیدا کرده و سپس به ۳۰ توکن با بیشترین تکرار را از میان تمامی توکن‌های متن حذف کنید. بدیهی است که stop word ها معمولا معنای زیادی همراه با خود ندارند. به این نکته در حذف آنها توجه کنید. در نهایت stop word هایی را که یافته‌اید به همراه تعداد دفعات تکرار به هر فرمتی در خروجی چاپ کنید. (۲ نمره)\n",
    "</font>\n",
    "</div>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 points\n",
    "from collections import Counter\n",
    "\n",
    "def find_stop_words(all_text : list[str], num_token=30):\n",
    "    \"\"\"Detects stop-words\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    all_text : list of all tokens\n",
    "        (result of clean_data(text) for all the text)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return Value is optional but must print the stop words and number of their occurence\n",
    "    \"\"\"\n",
    "    # TODO: find num_token top most repetitive terms and report them\n",
    "\n",
    "    stopwords = Counter(all_text)\n",
    "    stopwords = stopwords.most_common()\n",
    "    \n",
    "    \"\"\" most repetetive tokens:\n",
    "        0:16 - 17-18 - 23:26 - 29-30 - 33 - 36-38 - 40 - 43 \"\"\"    \n",
    "    stopwords = stopwords[:16] + stopwords[17:19] + stopwords[23:27] + [stopwords[33]] + stopwords[29:31] \\\n",
    "    + stopwords[36:39] + [stopwords[40]] + [stopwords[43]]\n",
    "    \n",
    "    return stopwords\n",
    "\n",
    "def delete_stopwords(df, stopwords):\n",
    "    \n",
    "    df.title = df.title.apply(lambda row: [word for word in row if word not in stopwords])\n",
    "    df.abstract = df.abstract.apply(lambda row: [word for word in row if word not in stopwords])\n",
    "    \n",
    "    return None\n",
    "\n",
    "# find_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 42589),\n",
       " ('of', 31727),\n",
       " ('and', 28929),\n",
       " ('a', 19888),\n",
       " ('in', 19701),\n",
       " ('to', 17475),\n",
       " ('for', 9976),\n",
       " ('with', 8382),\n",
       " ('is', 8046),\n",
       " ('that', 6556),\n",
       " ('we', 6272),\n",
       " ('on', 5893),\n",
       " ('use', 4948),\n",
       " ('by', 4863),\n",
       " ('this', 4728),\n",
       " ('are', 4299),\n",
       " ('an', 3497),\n",
       " ('from', 3481),\n",
       " ('be', 2732),\n",
       " ('it', 2644),\n",
       " ('were', 2584),\n",
       " ('wa', 2580),\n",
       " ('or', 2178),\n",
       " ('which', 2458),\n",
       " ('can', 2416),\n",
       " ('have', 2020),\n",
       " ('at', 2003),\n",
       " ('our', 1930),\n",
       " ('these', 1868),\n",
       " ('ha', 1658)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1 points\n",
    "    # TODO: read data file and do the preprocessing on the data (first clean the data and then detect and delete stop words)\n",
    "df = pd.read_csv('data.csv')\n",
    "df.rename(columns={'anstract':'abstract'}, inplace=True)\n",
    "df.drop(columns=['Unnamed: 3', 'Unnamed: 4'], inplace=True)\n",
    "\n",
    "df.abstract.replace({np.nan:''}, inplace=True)     # if abstract is nan replace with empty string\n",
    "df_original = df.copy()\n",
    "\n",
    "title = df.title.apply(lambda r: word_tokenize(r))\n",
    "abstract = df.abstract.apply(lambda r: word_tokenize(r))\n",
    "all_text = list(np.concatenate(title.tolist())) + list(np.concatenate(abstract.tolist()))\n",
    "\n",
    "df.title = df.title.apply(lambda r: clean_data(r))\n",
    "df.abstract = df.abstract.apply(lambda r: clean_data(r))\n",
    "\n",
    "all_text_clean = list(np.concatenate(df.title.tolist())) + list(np.concatenate(df.abstract.tolist()))\n",
    "\n",
    "all_text_stopwords = find_stop_words(all_text_clean)\n",
    "stopwords = list(list(zip(*all_text_stopwords))[0])\n",
    "\n",
    "delete_stopwords(df, stopwords)        # delete stopwords\n",
    "\n",
    "all_text_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>نمایه‌سازی (۱۲ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     در این بخش باید برای سامانه positional index بسازید.<br>\n",
    "    با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا چکیده آن، در این قسمت باید نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد.<br>\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word_to_dict(doc, word, docid, pos, col):\n",
    "    if word in doc:\n",
    "        if docid in doc[word]:\n",
    "            if col in doc[word][docid]:\n",
    "                doc[word][docid][col].append(pos)\n",
    "            else:\n",
    "                doc[word][docid][col] = [pos]\n",
    "        else:\n",
    "            doc[word][docid] = {col: [pos]}\n",
    "    else:\n",
    "        doc[word] = {docid: {col: [pos]}}\n",
    "        \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12 points\n",
    "\n",
    "def construct_positional_indexes(corpus : str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get processed data and insert words in that into a trie and construct postional_index and posting lists after wards.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: str\n",
    "        processed data \n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    docs: \n",
    "        list of docs with specificied id, title, abstract.\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = dict()\n",
    "    docs_store = dict()\n",
    "    for docid, row in corpus.iterrows():\n",
    "        for pos, word in enumerate(row['title']):\n",
    "            docs = add_word_to_dict(docs, word, row['paperId'], pos, 'title')\n",
    "            docs_store = add_word_to_dict(docs_store, word, docid, pos, 0)\n",
    "            \n",
    "        for pos, word in enumerate(row['abstract']):\n",
    "            docs = add_word_to_dict(docs, word, row['paperId'], pos, 'abstract')\n",
    "            docs_store = add_word_to_dict(docs_store, word, docid, pos, 1)\n",
    "            \n",
    "    return (docs, docs_store)\n",
    "\n",
    "\n",
    "docs, docs_store = construct_positional_indexes(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>مشاهده (۱ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "        این بخش برای مشاهده \n",
    "        posting list\n",
    "        یک کلمه و جایگاه‌های کلمه در هر بخش سند است.<br>\n",
    "         تابع\n",
    "        get_posting_list\n",
    "        با گرفتن\n",
    "        word\n",
    "        به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "            برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "        title\n",
    "        و\n",
    "        abstract\n",
    "        باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و چکیده به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b9b4e05faa194e5022edd9eb9dd07e3d675c2b36': {'abstract': [19, 35]},\n",
       " 'fc7c428f604d13604a1d62e8a3e1b393c730791a': {'abstract': [17]},\n",
       " '55f0b54f35949778487421325a1b6d0ba960b94b': {'title': [4], 'abstract': [5]},\n",
       " '4668306bc74576645c9cbe39a08f5f51a9bce4f4': {'title': [5], 'abstract': [43]},\n",
       " '8b9257f57d4aa43a5b1c7559a949e2cb7f9fd7de': {'title': [3], 'abstract': [43]},\n",
       " 'cd85a549add0c7c7def36aca29837efd24b24080': {'title': [3], 'abstract': [107]},\n",
       " '353d401a96939ff6c5836289077663a1f868ab19': {'title': [2], 'abstract': [10]},\n",
       " 'b163c8bc2963c5f1146ec168d11f1ca55d1e5e1d': {'title': [2]},\n",
       " '55ca7a2d71f7b82313b3a67d1713caad92a83ecb': {'abstract': [45]},\n",
       " 'f3d3bed63fb51315e2726837363ab3c9e89769eb': {'abstract': [0, 77]},\n",
       " 'a60680940d5486534b22ba2515eb8ae19dedf4f1': {'title': [2]},\n",
       " '82a99e0d626847fadb6be938f979a4aec573e9a1': {'title': [3], 'abstract': [0]},\n",
       " '804f87743641b18ae9a917a84e434e2313859fd3': {'abstract': [21]},\n",
       " '4fbe44946e0847ef82c86032840ad97b0521f40a': {'title': [0]},\n",
       " 'e60ff004dde5c13ec53087872cfcdd12e85beb57': {'title': [1],\n",
       "  'abstract': [20, 22, 44, 54, 75, 94]},\n",
       " '4c20e7f95448ca3c1042a6d7fa5fa15ec27e9aeb': {'title': [4]},\n",
       " 'a2e667e4382aaa8e02a17d0522c1a910790ab65b': {'title': [0],\n",
       "  'abstract': [22, 40, 94]},\n",
       " '20c7aaaef3588cf16ba83e92075c9832cf3b6ac6': {'abstract': [0]},\n",
       " '3f271912e46c926555a9078511ed681b58a5c27d': {'title': [0],\n",
       "  'abstract': [42, 81, 88]},\n",
       " '2f56d4ef0a9df1672699ecd787ffc9b3996026e2': {'abstract': [44]},\n",
       " '91e611c3e8705002438fb4439733e47ddec85b5d': {'title': [3],\n",
       "  'abstract': [1, 14, 46, 128]},\n",
       " '9a293c0602bd93d8f2b922912a8189c5c04df262': {'title': [5]},\n",
       " 'b32fb949edae3966fde7bc854099a659610ff14f': {'title': [7], 'abstract': [71]},\n",
       " 'a78ecfffe36114f1ccf69341e9949a9c407d3ea1': {'title': [0]},\n",
       " 'c4d652597e9996d238524d3ecee7762750667ea2': {'title': [4],\n",
       "  'abstract': [46, 74]},\n",
       " 'a0559ac561a6e816ee4294bf79797051281731ce': {'abstract': [55]},\n",
       " 'ac03e40cc705cbd9270d221eb23cf06460be579e': {'title': [0]},\n",
       " '6718e83fe2443acf30a1a9ee7b9accb540c77c28': {'title': [7]},\n",
       " 'eb42cf88027de515750f230b23b1a057dc782108': {'title': [1], 'abstract': [72]},\n",
       " 'ed74b9390eda908060fa3501b8f20a836ec98d63': {'abstract': [83, 120]},\n",
       " '51847659fff3a746f86a169ec8efa855b6286ca6': {'title': [1],\n",
       "  'abstract': [40, 145]},\n",
       " '3aa681914a7da79f7d7293f51a058eefe61c8bb7': {'abstract': [85]},\n",
       " 'cb6a82115777edee8d72feeb77427265b2a48e27': {'title': [4],\n",
       "  'abstract': [47, 55, 69]},\n",
       " 'befe794c29e0fe10d836789b358c65b049435692': {'abstract': [74]},\n",
       " 'f3dabf05a64e2bd5c40c5cccd518646af7da891f': {'title': [1]},\n",
       " 'e6c4293a92d50889eb202e49f85ef94dbb5f8afd': {'title': [8, 13],\n",
       "  'abstract': [13]},\n",
       " '0cbc480e0d380bbaa04bfb21a396c9e8da6e930e': {'title': [5],\n",
       "  'abstract': [30, 42, 97]},\n",
       " 'ca1d0a2c8656cd013d411b24b7ee540821a938c1': {'title': [6]},\n",
       " '883c5791a627d5bf679da1b5f871ff0d51792c6d': {'abstract': [18, 80, 102]},\n",
       " '779b489971775507fe6a39d98c52c4df56d9cce1': {'title': [1]},\n",
       " '4db22f69d5047659dc5f9c96be0b86a110bd4562': {'abstract': [3]},\n",
       " '492f26ecdab6a1f6665da64f861b0aedfc656d76': {'abstract': [40]},\n",
       " '2bcdc2897724e3bddf31c0fd1b9fb7799c3e2777': {'abstract': [39]},\n",
       " 'dc9c58793b1589132d49b0087fe493ff8e5863b7': {'title': [8], 'abstract': [5]},\n",
       " '2a793b8434174806bcfc3a367ad205613516818a': {'title': [1]},\n",
       " '33526226231cce669317ece44e6af262b8395dd9': {'abstract': [0]},\n",
       " '35ead69983132d6efd7548788c66f355a5a9f9ab': {'title': [3]},\n",
       " '0a78085721f70d82c1284c124c3137bb7c2b34e7': {'title': [2]},\n",
       " '55e0b7cae68a817d91756d02c7eb04a7079d7b5a': {'abstract': [0, 33]},\n",
       " 'afff02fa27bdeb0e2dc658d1ce43af6d3f407ff5': {'title': [1],\n",
       "  'abstract': [53, 56, 75]},\n",
       " 'bd36f1a9b111c535281c32ed600ddd6c4b71364e': {'title': [3],\n",
       "  'abstract': [42, 55, 96]},\n",
       " '0374f9e9541660a35881f0b09d5ee4c8eb5c921f': {'title': [6]},\n",
       " 'a664f7256ffb3b81bf8532f94d6d70122f303a96': {'title': [0],\n",
       "  'abstract': [69, 84]},\n",
       " 'bb4a9650ca3946c70a7e92007cc61dc0dfd75522': {'abstract': [32]},\n",
       " 'a389216b359c706418bf6611ef63bb083c15e9cb': {'abstract': [71]},\n",
       " '922d93d253fc8c55f559586a328ae999ef749cdd': {'title': [9]},\n",
       " '7afe3a449f0dd31ec08aa9748f94079684f899e6': {'abstract': [47, 75]},\n",
       " '1e58920c4235fdbfc22534c92938f22d1728f0b9': {'abstract': [34, 95]},\n",
       " 'a7c3bdca5b1c2ca860a4d24d0b35f86b056bb30d': {'abstract': [94]},\n",
       " '1ba07035f83792154e77ed35d842e04de8a1eeeb': {'abstract': [21, 29]},\n",
       " 'af24864ab4c654a44637098cb615c8b03f39b97f': {'title': [1]},\n",
       " '261b15e8a013e07d6aaf80af930f4748ce35ca5f': {'title': [1]},\n",
       " 'd29444b1873c206fba4ade481c33e9d80ecfb7c6': {'title': [0], 'abstract': [19]},\n",
       " '1a2b0383369172e08d91bc60fb70a9a988c317cc': {'title': [1]},\n",
       " 'cc2defd5616b98eda74cb908d66498692753f164': {'abstract': [31]},\n",
       " '447499a1b18a71a914964f9bd782ac8cfbfcb739': {'abstract': [27]},\n",
       " '5fdb3533152f9862e3e4c2282cd5f1400af18956': {'abstract': [0]},\n",
       " '2aac28eedb11ab7a15d2b095b276547fa03db580': {'abstract': [58]},\n",
       " '02da8e322c2b6fb00a6be642c61426c68e34349d': {'title': [0],\n",
       "  'abstract': [67, 116]},\n",
       " 'b2d1008ec02fc29478737daef710ba4436b5aafa': {'title': [5],\n",
       "  'abstract': [10, 22, 36, 63, 81, 102, 129]},\n",
       " '5311c6e3e4a08bfec11a600888fa8537a9ee2e97': {'title': [0],\n",
       "  'abstract': [25, 48, 55]},\n",
       " 'c14e77fc10f133358903019f45132d694682ab88': {'title': [0],\n",
       "  'abstract': [8, 43]},\n",
       " '0f37b083cfd3d113f5c26dbb3620d246b9a40292': {'title': [6]},\n",
       " '7db1e11805dc94b2c888625918f8ba810c4a7b33': {'abstract': [33]},\n",
       " '49e90a0110123d5644fb6840683e0a05d2f54371': {'title': [3],\n",
       "  'abstract': [58, 67]},\n",
       " '7b65f3e60065cff562f45c6c20f4e8a27949f18d': {'title': [0]},\n",
       " '2e2b189f668cf2c06ebc44dc9b166648256cf457': {'title': [5],\n",
       "  'abstract': [1, 44]},\n",
       " '188452cb4bdfcf2b182c3eea6b2d3f15eb5ffa73': {'abstract': [18, 98]},\n",
       " '6c8a56ae495e5c8871061d1cd0f863d174f5e2ce': {'abstract': [16]},\n",
       " '2e535b8cd02c2f767670ba47a43ad449fa1faad7': {'abstract': [26, 28, 50, 76]},\n",
       " 'a52d53f84400b10aee8d235d70852bc68ca0d32b': {'abstract': [17, 125]},\n",
       " '2515d60eadc7c4d56e463b2dcff6dd7a6de68fe1': {'title': [6]},\n",
       " '5411a036cb5757915f2268333b6caf91132867ad': {'abstract': [10]},\n",
       " 'f98dbe64ed6fa8925048291fcceb625d704fb294': {'abstract': [20]},\n",
       " 'f31ff4fbf1582c090ec1ac247add43f8ff37a839': {'title': [0],\n",
       "  'abstract': [52, 108]},\n",
       " '96f535751c4e6e4fe5f2cb00d186fedbad7efd71': {'title': [0],\n",
       "  'abstract': [92, 101]},\n",
       " 'b0204e6c7301281ec8460e5251cd7c3c83c40883': {'abstract': [31, 107]},\n",
       " 'b7f993243d20310fc7eca06dc1cf0ce3550feae9': {'abstract': [72]},\n",
       " 'b0bf93b1ef16ace689123b26ffc7b17ac9894582': {'title': [8]},\n",
       " '0b26e182cde3c28d79119424660d05520428ec8d': {'abstract': [2]},\n",
       " 'fe0776a8ec8d0d1146f3c6d90d3f76d8e443f695': {'title': [5]},\n",
       " '5eee61e47d55ed1ba90a00c91fcb3673d91e042f': {'abstract': [44]},\n",
       " '7d8d989194afb78158206b9803907e2c0bd228bb': {'abstract': [90]},\n",
       " 'd602137d38d4fad9e6db07d911fbf3551894d441': {'title': [0], 'abstract': [0]},\n",
       " '117ac7c5d42bfdbe83432672db05a2de08dae113': {'abstract': [1]},\n",
       " 'bf05819fa9366b3b67b0d86347b7b16fee88eb61': {'abstract': [1]},\n",
       " '79cc853e623ef1ae6bef9219b4d8900be7b5d917': {'abstract': [31, 99]},\n",
       " 'e036d15bbb4cc3176b07e2cbfe842165ee841a26': {'abstract': [0]},\n",
       " '1de7d838cfce1c3bef65b134bae3d00b59eddd71': {'title': [4]},\n",
       " '2abde28f75a9135c8ed7c50ea16b7b9e49da0c09': {'title': [1]},\n",
       " 'da26630c9d931952f1d68308148f1289f5332607': {'title': [9], 'abstract': [8]},\n",
       " '0695280cb4fdeb4eb8c0b01dd04a3a5f3bc6ef56': {'title': [3],\n",
       "  'abstract': [3, 34]},\n",
       " '60d55fcfa445184f6b2c711138899b7c2114a94e': {'abstract': [48]},\n",
       " '245eacda2e4cbac0b8157bc274d2417aec0da13c': {'title': [0]},\n",
       " '1d3485be8a6ca0ce83f2c9e968547cfc24240ed3': {'title': [8], 'abstract': [10]},\n",
       " 'e10a130ac1404572b9cba1d636b34169799ecbcb': {'abstract': [49]},\n",
       " 'fc8f2264c67107fe3ae5c084d436075afe6486b8': {'title': [5]},\n",
       " '284f596e438da346acec45b59d4c10850632ed66': {'title': [0]},\n",
       " '335cce41a59ff311213da55dfcd116aefb97fdb8': {'title': [3]},\n",
       " '527fc9235ebe59f415af8af1aba3db4325be1597': {'title': [0]},\n",
       " '821f2cae0304cd3714ebd7ca31fa68d4029f1d3c': {'abstract': [32]},\n",
       " '763be1dffc2aec2637ebea505a7612bb6843521e': {'abstract': [1]},\n",
       " 'b4a4e6fe63426b9fb4393c939792da8e7c4a1a59': {'abstract': [71]},\n",
       " 'edacb19ef8c6ba532639b587904f67e648910f47': {'title': [5]},\n",
       " '3e6c84302b2b56cf8369253d6168b852d0aa1fd6': {'title': [0], 'abstract': [119]},\n",
       " 'f86bbcff0d2cdcf52049373bf1b67b0d4b6d3e7e': {'title': [1], 'abstract': [14]},\n",
       " '360e574d95ea8d5e3938f756badd1d5993974159': {'abstract': [37]},\n",
       " '96c62b83c2dc54b22a126b53efb201fae61b3983': {'title': [2], 'abstract': [3]},\n",
       " '04625f7a0f84f0466f690d7b0bc672c7c90c193f': {'abstract': [28, 32]},\n",
       " '4c8f50cde3f2ce221e3c6d5171420b96c144df27': {'abstract': [5, 24]},\n",
       " 'bfe284e4338e62f0a61bb33398353efd687f206f': {'abstract': [32]},\n",
       " '8f1a8b82c7be223f195b4f03ffa1943391fd428b': {'abstract': [11, 28, 100]},\n",
       " 'e2c477de72bb7718f5304c6f38457fda9c8334b1': {'title': [0], 'abstract': [4]},\n",
       " 'e56b10f7cd4bf037beac84da5925dc4544fab974': {'abstract': [2]},\n",
       " 'ca120273f3cd35d565ef3d9afdcec80adef19d37': {'title': [1],\n",
       "  'abstract': [24, 75]},\n",
       " '4054f15b807dcdc4d2112166f449d70ab7b1a8da': {'abstract': [25, 97]},\n",
       " 'f215ce917c06fcfb21af5cc0283e008e6c391816': {'abstract': [77]},\n",
       " '1a442ec8ba0235c7180b29c39a507ee57841fd7b': {'abstract': [7]},\n",
       " '7ac13836ed80c668eff5ac12903a0cd5c237e27b': {'title': [1]},\n",
       " '14b8737bd7870fc61346123fffc9755bce7254fd': {'abstract': [3]},\n",
       " '441ff323c92331e655ce9ff896773fc00b55089a': {'title': [3]},\n",
       " '276f151c8e2eaf17541966c49c0aabc96db8f64d': {'abstract': [3]},\n",
       " 'f9c7fc60267d9541fa396e2ba63ad78a2dd995eb': {'abstract': [94]},\n",
       " 'a93aaf608f3bccb8174cd52ff32dcd9ffb25b423': {'title': [1]},\n",
       " 'b2c1fb445d25de3a26482440cd4f977e2f1cde46': {'title': [3],\n",
       "  'abstract': [42, 58, 84]},\n",
       " '64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b': {'abstract': [31, 58, 98]},\n",
       " '5d7cf0d50d5fd373bafef6e3ab554936a54e3cda': {'abstract': [14]},\n",
       " '0276aecb97ffa1f7f279e2768abb3484933eacfb': {'abstract': [19]},\n",
       " '13111b3795bf32fc1ba508e6ea615031751b33ee': {'abstract': [65]},\n",
       " '367f2c63a6f6a10b3b64b8729d601e69337ee3cc': {'abstract': [0, 34, 89]},\n",
       " '531909281a0c92c738d9abfe7ad9a3630f9a2caf': {'abstract': [68]},\n",
       " 'c827842512a843ae3d27db554b954a4f782a633a': {'abstract': [15, 62, 87]},\n",
       " '713e881b5c3134debf934026edf6f0ba3cb42c3c': {'abstract': [50]},\n",
       " '0fc190033ec2832ed65dfac7a19bdb8a270fb6eb': {'abstract': [8]},\n",
       " 'b8c6fccfd7190cdca8647f5886716a4f56d9de4b': {'abstract': [0, 15, 33]},\n",
       " '8336e55d4b31a4ba6c3a128d46eb0289aab19789': {'abstract': [81]},\n",
       " 'a4040560bed2818f9e64293c462627858d99cf3e': {'abstract': [74]},\n",
       " '96baf66b1ead911d18b64ad674c9512e189c290b': {'title': [2]},\n",
       " '40cec58d760940f62975e0d5e99632e28df0b5d0': {'title': [3], 'abstract': [19]},\n",
       " '1e8a5d2c359ead7ab2e55e1d492a7ed1e2808d0c': {'title': [0],\n",
       "  'abstract': [15, 60]},\n",
       " '3cf6300ea6f90876b14dc56242cf57ce0d36d8b5': {'title': [6], 'abstract': [51]},\n",
       " 'b5b7757491d8f60eb68e9b5f64190443cdcf55f8': {'abstract': [68]},\n",
       " '6b2062812d2e353ea884a0cc077e9f6c73351423': {'abstract': [0]},\n",
       " 'a1de21ce6dd0ff6327e6cf5f124cce8db197991f': {'title': [3],\n",
       "  'abstract': [71, 78, 103, 124]},\n",
       " '13c2abd38e3f9f5d3eeab177d43a1e0e1a6e64e8': {'abstract': [18]},\n",
       " 'e157466239e336a3146086a91da58a7938de977e': {'title': [8]},\n",
       " '39c5740304b5f4072f92e4e012a4b57e7bc2e817': {'abstract': [11]},\n",
       " 'f6a80bfb5fb23fdd940f1bf36ada123248346ade': {'abstract': [74]},\n",
       " '46917e427a7aa1f4d415f8620c43ba4845efd435': {'abstract': [49]},\n",
       " '9cba6d03d13e18c0100ad9e7858eaa0ec2d18ee1': {'abstract': [0]},\n",
       " 'b9e98f630e8eaf77ddcd0f80d1360b611ae61e70': {'title': [0], 'abstract': [11]},\n",
       " '3d5a4ac2982fea9b2a675916639c9e892a89a628': {'title': [5]},\n",
       " '0ea4076406bdef0225d095b45d20ee457a637eb9': {'title': [1]},\n",
       " '1b0cbf49580883157dc2e674e64270b16656aed3': {'abstract': [0]},\n",
       " '3ab03674dfbb3762ccf3f842e840ad25ddb79be3': {'title': [2], 'abstract': [0]},\n",
       " 'bd25bbec1c321ab28846b04ee6d269e10e18f54b': {'abstract': [164]},\n",
       " 'c5adb029d021946dc4347277f8d43fe509cb96b0': {'title': [17]},\n",
       " 'a2843523d5b96c380220ce1d6da133abeabcfe30': {'abstract': [64]},\n",
       " '3b0fb765716ef6861a84abffcbe40643857c613b': {'abstract': [2]},\n",
       " '255782ed38a221a43ddb5e8c63e8c77bf26382e7': {'title': [1]},\n",
       " '5e159c0f1f56ebc51e8d44c18cf579a891ef1c5f': {'title': [1],\n",
       "  'abstract': [37, 62]},\n",
       " '81a4fd3004df0eb05d6c1cef96ad33d5407820df': {'abstract': [0, 56]},\n",
       " 'ab6d2e28f51012b1968b8b460624178d1cf47a5f': {'title': [4],\n",
       "  'abstract': [12, 34]},\n",
       " '8d3e9f744ee2514eac27d9288581ba4cab131f4a': {'title': [0], 'abstract': [69]},\n",
       " '498e003901f8287e89e5064477cd22dd47e49d61': {'title': [3],\n",
       "  'abstract': [3, 86]},\n",
       " '80be914311228dc3065f2f75f3de71459f3b1043': {'title': [0], 'abstract': [46]},\n",
       " '3060a4828f4fd0a89e934db5b74bfdef95141d80': {'title': [6]},\n",
       " 'ccd8d46f4c5e59fba944385ad34ef96de950582f': {'title': [3],\n",
       "  'abstract': [23, 54, 63, 79]},\n",
       " '4a8cdb0175b37e13de8bc64cf32b9e784aa0808c': {'abstract': [20, 95]},\n",
       " '1e70d24649c5572b03ebbac45a784bf7698e56c7': {'title': [3]},\n",
       " '6d4a87759917132913319960389f17fa1fe8b630': {'abstract': [5, 84]},\n",
       " '11b64286259c92cc82ef2ccc05c492f1a84ece82': {'title': [0]},\n",
       " '59d8c68de09da69a608ceb149f40114f5538c5b1': {'abstract': [30]},\n",
       " '2c88c3d8bc68858e4c9ba58ec3d71454e94eb96e': {'title': [6],\n",
       "  'abstract': [46, 151, 172]},\n",
       " '832c0fb24669a7f7502510a70c4f3ddca42d45f9': {'abstract': [166]},\n",
       " '51ba3b33f445199d9f3cddb5b00c7e2927199b0c': {'title': [0],\n",
       "  'abstract': [1, 56]},\n",
       " 'a52a3aac68f4fd67b67ccf6839765403a101f681': {'title': [3], 'abstract': [5]},\n",
       " '0c2a4ee3e6b8cf1485fb1ffee00dd4f5a9fc4bef': {'abstract': [131]},\n",
       " '21117380118ddce47b3c515c5228372c513e61ba': {'title': [0]},\n",
       " '6c96c2d4a3fbd572fef2d59cb856521ee1746789': {'abstract': [2, 32, 147, 155]},\n",
       " 'd5ce8f2c3c4be1fbae610a4b81eee85c41337150': {'title': [0],\n",
       "  'abstract': [44, 58, 68, 92]},\n",
       " 'ab096e819b0fcb82edea56b9d28fd5500c16906b': {'title': [0]},\n",
       " '3937f1ad451462fbd9994c37ad22af841be9a3f8': {'abstract': [43, 61]},\n",
       " 'e3c7766bfb93a488218ff98d7420dc20cead969d': {'abstract': [3]},\n",
       " '2867f915f7419915909e60b3f9f8a29ada9f10d2': {'title': [5], 'abstract': [44]},\n",
       " 'e56320ed18e4fef4cbec9a35a1d5e7fe9c05b7fb': {'abstract': [15]},\n",
       " 'f3f1ea39c55e9c85a68fe50450cd7dfa6515e55c': {'title': [0],\n",
       "  'abstract': [52, 69]},\n",
       " 'f9ad25505d7b491b2d14d4178fbbe95ecc0c951f': {'abstract': [17]},\n",
       " '924385a663f9bc3a5f6979f72b43e069e06c263c': {'title': [2],\n",
       "  'abstract': [3, 35, 49, 55, 70]},\n",
       " '8d35663a80199b173d8cbd12dbf2300a9f86a021': {'title': [4]},\n",
       " 'd5d571db338064b050044f936ff6914c79c5e4a0': {'title': [0]},\n",
       " 'b6721d9173ad140c77cdf98a919bdb40baf42c0c': {'abstract': [56]},\n",
       " '7d8d6d30ce3bd383f947d6d88e22e00e809065f7': {'title': [2]},\n",
       " '802168a81571dde28f5ddb94d84677bc007afa7b': {'title': [5], 'abstract': [0]},\n",
       " '01c0348a895d87aaad05b7705a62478f45b70449': {'title': [1]},\n",
       " 'a22ee3256e28c7551d2fd7aedb965760457e8e6a': {'abstract': [21]},\n",
       " 'f3dc56a28ef71987f3999ba157f91cf351fb0a5b': {'title': [2]},\n",
       " 'ef8ab2a0be51a0cd04c2c0f01adfae956a2a84af': {'title': [3]},\n",
       " 'f44f5a877b34788afb3e0fc49c634faff69ab152': {'abstract': [76, 106]},\n",
       " '6d8052588c62e5bf2a073ae414867a78784ff663': {'title': [1]},\n",
       " '3c6599052623542f6e21a2cc45a6fde4fb2dc374': {'abstract': [25]},\n",
       " '25e9fa483a048607131a5a0e3287e8f457fb4807': {'abstract': [0, 13]},\n",
       " '82b1b54e0b6134c4f072d2fee9692f3fd636dfa2': {'abstract': [33]},\n",
       " 'b71cbb8150760e1724a9c5dec78af4d0832e4236': {'title': [6]},\n",
       " '505e72b8d60e987e89b93fdd98859b857ca94207': {'title': [0]},\n",
       " 'f08c5a30f1f0c9d79861f7eb993bf89fa9ef1074': {'title': [1]},\n",
       " 'af5e811897a5ab945d58624d576debd82e03fac3': {'abstract': [1]},\n",
       " '6705972cf9104d76fd7423dc414f3915f41ae783': {'title': [7], 'abstract': [119]},\n",
       " 'bbf052271c70dd926cd230bb5640cdcdfeccf1cc': {'abstract': [13]},\n",
       " '6bd36e9fd0ef20a3074e1430a6cc601e6d407fc3': {'title': [1], 'abstract': [98]},\n",
       " 'd05ff914131723b3287b91c169d579f13fd01209': {'title': [5]},\n",
       " '74ec403743704c1d72b83c96d422996bcf232b1f': {'abstract': [32]},\n",
       " '901d678c367247e84b39a9ae5f913c5fbd958a0e': {'title': [1]},\n",
       " '08d0ea90b53aba0008d25811268fe46562cfb38c': {'title': [2],\n",
       "  'abstract': [0, 105]},\n",
       " 'a469cb515cd9a2e1923a7a9b3b3fca3690881b19': {'title': [1], 'abstract': [61]},\n",
       " '28960bae9bbae36e25d1018ab7d6e8b258a903a0': {'title': [6],\n",
       "  'abstract': [24, 45, 79]},\n",
       " 'a11124688cdcee011043d7e4cdaf10332a533946': {'abstract': [14, 100]},\n",
       " '424561d8585ff8ebce7d5d07de8dbf7aae5e7270': {'abstract': [95]},\n",
       " 'f29df408f1407c60b0f85dd595a883de7c66e763': {'title': [5], 'abstract': [37]},\n",
       " '4f8bf7de1d0632ebc5e9d5a1da14c63ad3b7124f': {'abstract': [13, 64, 73, 115]},\n",
       " 'a06a8ca70096e567e5cf1e433cc99ac1d519c4d0': {'abstract': [18,\n",
       "   38,\n",
       "   48,\n",
       "   57,\n",
       "   85,\n",
       "   97,\n",
       "   114]},\n",
       " '5d68bbeb2d293cbda75994ca5040fd083665e82e': {'title': [1],\n",
       "  'abstract': [54, 65, 99]},\n",
       " '27eec716b3d5f2253ced3d6a30eaaa5558ae94d6': {'abstract': [98]},\n",
       " '5bdbadc741ce13762b7c914c1514124404c46211': {'abstract': [122]},\n",
       " '1cecbb9400b0d8b4342e6c25599b0e6a53a0ed41': {'title': [0]},\n",
       " '5f58c4eef5728d53d124de6226c134d2e28d846c': {'title': [0], 'abstract': [6]},\n",
       " '939f575c8669331ce29f8ba4415dc2ddd06d5a5c': {'title': [1]},\n",
       " '8275caf3674a37343fc6486ec354f0ebdf44cd99': {'abstract': [14, 23]},\n",
       " '08f2d3b5a393d9b7aa47a5edf81d6f8604688e07': {'abstract': [40]},\n",
       " '3ef82c1cf7af5988afd6ee19cd595556e0085a76': {'abstract': [8, 18, 37]},\n",
       " 'd939289250e4c635ae2d3e80834862d392473100': {'title': [5]},\n",
       " '7dbc187624870198c9713483a3d225be22fb3fe8': {'abstract': [108]},\n",
       " '20f77d34ab1aec7d0a6e613a740aff7ec7fbf55a': {'title': [6], 'abstract': [40]},\n",
       " 'daf74c34f7da0695b154f645c8b78a7397a98f16': {'title': [1],\n",
       "  'abstract': [11, 31]},\n",
       " 'e38e70580acb204c05096de8da90b7ab1d4bdb6b': {'abstract': [25]},\n",
       " 'e503e08d63ee02fd83f549bdd9ddc8b58d5998df': {'abstract': [40]},\n",
       " 'd716435f0cb0cac56237f74b1ced940aabce6a2b': {'abstract': [11, 48, 79, 129]},\n",
       " '9702b3da2cca9b9f9ca53c0be1de09b644a9e34f': {'title': [6]},\n",
       " '9fd8e5cb7958c1b005096e95ba15d0d92cdfbbaa': {'abstract': [14]},\n",
       " '6ff5533db1ae20f72f471b97965a8b67f05e62b6': {'abstract': [83, 118, 129]},\n",
       " '5c6199859eb8b4ebd4188c6587f6ad8e6448a983': {'title': [11], 'abstract': [25]},\n",
       " '602645d132457923c98794db8f6d4a042714620e': {'abstract': [1, 39]},\n",
       " '7e1b5de9e0a8767a3d4071ea1299358dc067f7e3': {'abstract': [120]},\n",
       " '24cf86a418c9471e8001961c87697c825f0bba8f': {'abstract': [9, 74]},\n",
       " '3e0cb50eb39ec30c9a791bf2fc4a7878d0a2478b': {'abstract': [82]},\n",
       " '930a3aabbaf3476807dd0fa0cd2830e371cb5b1c': {'title': [0]},\n",
       " '56c6cc360cc60f20884851daa8943d16219fb5c2': {'title': [4], 'abstract': [78]},\n",
       " '36dc816c0313d1ed1be6ee5effebe08f92a41836': {'title': [0]},\n",
       " 'b4d612f602817f4a7d1ea33d7b03253394cf5cb3': {'title': [1],\n",
       "  'abstract': [12, 83, 97, 108]},\n",
       " '625c3decc31a2c3d7ca9f5f84450f701798c28ac': {'abstract': [83]},\n",
       " 'd19701ccfb9170cc4b8b7856ba186ed3db7eb56d': {'title': [3],\n",
       "  'abstract': [96, 114]},\n",
       " '34ee498a8dab7d786ff8ae1b648cfcfdb12fcf16': {'title': [8],\n",
       "  'abstract': [64, 77]},\n",
       " 'f374d4bdf63fa1d2358a5559dbea32b576968d5c': {'abstract': [2]},\n",
       " '4ca058c44bc35930b2a3cef88018f49336392bd6': {'title': [1],\n",
       "  'abstract': [74, 88]},\n",
       " 'b0c065cd43aa7280e766b5dcbcc7e26abce59330': {'title': [1], 'abstract': [3]},\n",
       " '590d4a5e92d9f00621aec16fcb5df16d1912b4ec': {'title': [2],\n",
       "  'abstract': [3, 52, 88]},\n",
       " 'fd81880d09fa9997be8a0fccd5f1bf3fc4eb3fcb': {'title': [10]},\n",
       " '2a48d2b673689e4434fc6b86965ca2a91bb40103': {'abstract': [8]},\n",
       " 'fef2135b3ae7b27ab28ddf41a943bd2ddc5d5113': {'abstract': [47]},\n",
       " 'eab422b69605e047597a743ebb657b2728ad7f8e': {'title': [6], 'abstract': [16]},\n",
       " '6c30e84311d6ed8a41908bd4ce0ebd7bcc08cc83': {'abstract': [45]},\n",
       " 'ddf52ceb8452344f4b93c87c0ec97834cbdd7230': {'abstract': [43,\n",
       "   62,\n",
       "   96,\n",
       "   237,\n",
       "   322,\n",
       "   332]},\n",
       " '42488e3a61d577e544e554b206a17e408f4cd765': {'abstract': [31]},\n",
       " '67c4528de75ae743d23a4eea5c03316a7a8cf9d1': {'abstract': [63]},\n",
       " 'ede7829b3f057a874c513919d19307e2b60ead23': {'abstract': [65]},\n",
       " '8381157eae4fbf8908d0312a9642f8e69e944449': {'abstract': [40, 51, 64]},\n",
       " '76b1768c4185b4b6e525e797be137964ffd46cd5': {'abstract': [30]},\n",
       " '4acde8d06cb9d7db0a793169d2bfe007fcd62301': {'title': [1], 'abstract': [47]},\n",
       " '2c03df8b48bf3fa39054345bafabfeff15bfd11d': {'title': [0],\n",
       "  'abstract': [94, 103]},\n",
       " '1a3c74c7b11ad5635570932577cdde2a3f7a6a5c': {'title': [1],\n",
       "  'abstract': [2, 21, 65, 117]},\n",
       " '88e8db0971a8390d4b197e44242e87c50ef3e1e1': {'title': [1],\n",
       "  'abstract': [27, 39, 75, 148]},\n",
       " '30b25894928fa69b748fe504e37ce3656ff5d71b': {'abstract': [123]},\n",
       " 'dd8ae7e8b72b0bbb11a4d9537a0f1b9f051b0793': {'abstract': [24, 88]},\n",
       " '1f83ce6b690b452d0b03ef6e4b43e96ff9ee6d2a': {'title': [1]},\n",
       " '2faf72ee3ff950009a10020f720db336bf5e63a0': {'abstract': [1]},\n",
       " '98d2656eb36056a81462ad175656a8e8730bf13f': {'title': [8]},\n",
       " '5e785ebb971ef0c908ac4e408206e82e76388cbf': {'abstract': [18]},\n",
       " 'aa14c359aada2ce633f794ddc40b584ece79ddf5': {'title': [10]},\n",
       " 'ceee1196f166a41c3c6cbccd03a9e6b8a9bfa574': {'title': [5],\n",
       "  'abstract': [11, 56]},\n",
       " '25e3cad9f9f7191e5057975c9c2d3f2fda24ff96': {'title': [4],\n",
       "  'abstract': [10, 29]},\n",
       " 'ef2e936aa7b6615f5e7cdb642428da5defb57c07': {'abstract': [0, 13]},\n",
       " 'f508919ebbf44b250159e9c97f749610a5ae670a': {'title': [0],\n",
       "  'abstract': [28, 77, 81]},\n",
       " '28227bc2f709616b606e10f72748a03cfc47cdbd': {'abstract': [66]},\n",
       " 'f08f76b54c1b6ac1c760f0d9389e6eeb1ce8204a': {'title': [7],\n",
       "  'abstract': [29, 82]},\n",
       " '9a3855ac64358d95386cac5120929c88197e26dc': {'abstract': [108]},\n",
       " 'be12d320fdfa1319a8a8379b9e637ffbb6450f97': {'abstract': [83]},\n",
       " '17af9510a38e4dec93398707f11d833c8af36254': {'abstract': [6]},\n",
       " '77d30cf9a34fb6b50979c6a68863099da9a060ad': {'abstract': [62]},\n",
       " '7efb4f8ff9aa6f5e9691e9392b539298ef46ca04': {'title': [2]},\n",
       " '9f25e139d99cac8957c758325bf77dad29d65073': {'title': [4],\n",
       "  'abstract': [8, 68]},\n",
       " 'a2b231bc08cf3d9ed66eb60e6e45a0d84e94c090': {'title': [2],\n",
       "  'abstract': [2, 36, 46]},\n",
       " 'bb51ca71833d42fa58f9adccb2296bdf665cc158': {'title': [8], 'abstract': [18]},\n",
       " '642d0f49b7826adcf986616f4af77e736229990f': {'title': [0, 3],\n",
       "  'abstract': [20]},\n",
       " 'aedbb2a8c27d1460345df0dcdfdc1f21e2225815': {'title': [1], 'abstract': [52]},\n",
       " '28293e2b07acccc222f5b0deca15105430759a92': {'title': [4], 'abstract': [30]},\n",
       " '4dfbc372c5e127abe7989e9c4ef75daac7370cd2': {'title': [6]},\n",
       " '8edb469ef694e6259a97e88a53440338d3865ebc': {'abstract': [4, 81]},\n",
       " 'f9c16fca2662a82428a977e09db0ea02a74911da': {'title': [4], 'abstract': [38]},\n",
       " '742c3acf2dab95cabaca8bd8aa27dcc03863ac2b': {'abstract': [22]},\n",
       " '85a854a51833b16994f5af77a7419acb2bc78942': {'abstract': [39]},\n",
       " '5cd20c68a8c8554b89f762e0a5eac2bda03059c4': {'abstract': [0]},\n",
       " '29edf1f3bf744aa1a1b7bea3c1fb455bad8fc898': {'abstract': [5]},\n",
       " '8407167036a2795944138ac909e16adadec13668': {'abstract': [11]},\n",
       " '0c35fd52b312eb3881a08b04b67b4b25f473ab8e': {'abstract': [58]},\n",
       " '53b047e503f4c24602f376a774d653f7ed56c024': {'abstract': [5, 107]},\n",
       " '5a7033e9f2c459a9162e16d607297d88c60f5236': {'title': [6],\n",
       "  'abstract': [47, 51]},\n",
       " '9358d9e9afbc1eaf6b2f2042a8adc573556f566e': {'title': [2], 'abstract': [80]},\n",
       " '1167a9431e3743e16ab7e38bf60bdf63a278f7e9': {'title': [6],\n",
       "  'abstract': [26, 70, 90]},\n",
       " '64d4fcefa471fba93b910480c69f0127895b3512': {'abstract': [31]},\n",
       " '8d68eae4068fca5ae3e9660c2a87857c89d30f73': {'abstract': [0]},\n",
       " '4948596c4cb32a6ce1ce403a580c7fc4a51fe4f7': {'title': [0]},\n",
       " '549cc49a43feaae4bc64da81e7299f0a78e6858b': {'title': [0]},\n",
       " '82e597010ee1bede0b50f7ff8c6a249eba3a67a8': {'abstract': [2]},\n",
       " '6a51ddf8788de3002ec15ca7b30d2195c08c8929': {'abstract': [0, 23]},\n",
       " '89890e89a1ced045193fff89f005cd7cafa76d9c': {'abstract': [16]},\n",
       " '3dda2a28f8b7ddb89bfaa3bf1f1577d30afc7f2a': {'title': [3], 'abstract': [9]},\n",
       " 'b4919d8160d1c5b47f0ecbe30fa2237fecc7ba25': {'title': [3],\n",
       "  'abstract': [57, 79]},\n",
       " '1a9314776fbf9dd92649be5c2583b7ff92d79b56': {'title': [0]},\n",
       " 'e44ef7be16ac01dfde6597b094bcbe4f81f771be': {'abstract': [34]},\n",
       " 'b4a94f4bd9842fbb72dc491665cbe70316aba431': {'abstract': [13, 63]},\n",
       " '796ca5c0c43614a94167fd48279bb9c6eaef49e6': {'title': [4]},\n",
       " '028b15f4e4e6e12ab48385cd274cecaf887e80f5': {'abstract': [128]},\n",
       " '3a7f661d157cfb689bb35969e1a0fccccf8ba698': {'title': [7],\n",
       "  'abstract': [27, 102, 118]},\n",
       " '601106659dc89cb329e8a045ef6ef288fe37b0dd': {'abstract': [8, 105]},\n",
       " '7a0324f4a44047260ab3ebbd87a3f4744f1ea945': {'abstract': [5]},\n",
       " 'fef6f1e04fa64f2f26ac9f01cd143dd19e549790': {'abstract': [40]},\n",
       " '1b277f6e55e328c865c83518594fe3b5bd569205': {'abstract': [28, 45]},\n",
       " 'af3b7e1e2921aa68f6e00e07589c0d0f585e9f76': {'title': [4]},\n",
       " 'b3ea79473e7438ae3d551f79a84207f8bd0e830d': {'title': [4], 'abstract': [38]},\n",
       " '6dc580ca568e8685854e4229881d74c69d57c8eb': {'title': [10]},\n",
       " '27b6749039e79be9357356f3abe232dc645b6623': {'abstract': [86]},\n",
       " 'b051f7540ca1c2c5d1f8e6eb9ada449d2933ac45': {'abstract': [0]},\n",
       " '14a657db8a4e95e07379494828717027965022dc': {'title': [3], 'abstract': [86]},\n",
       " '6541eba27e35e43c009cf775df96b679eaf49692': {'abstract': [4]},\n",
       " '3fcbf108daffa73ccdead58ff7662d6f241339de': {'title': [3]},\n",
       " 'd2e4587744a89bad95fea69e08842cad6c8ff0dd': {'abstract': [6]},\n",
       " '8df989b1640e841538f8a12cc973b565b0712757': {'title': [2]},\n",
       " '877150193febd35691287324bb7c436fd8d75acf': {'abstract': [19]},\n",
       " 'b64a9491b5f64e68a5c48963feff50417510c799': {'title': [2]},\n",
       " '13103b3bd4808ffd8dc46c6a77cec376eb401736': {'abstract': [27]},\n",
       " '20484b42bd462ef016de99ffa71f67c6c295ed8a': {'title': [7]},\n",
       " '05a230ed244e2d262f54a2a8509dd5b7aabdeab2': {'title': [1],\n",
       "  'abstract': [77, 134]},\n",
       " '7494f7fd1cd30ca2a2753b69271ed75b3967cb70': {'title': [3]},\n",
       " '1cae417456711c4da184f5efcd1b7464a7a0661a': {'abstract': [54]},\n",
       " 'd14ac2acf1b18e815385c631216eb4ee3a4fc842': {'abstract': [291, 295]},\n",
       " '469636449a996299e43176a6ecca36dcbbb9e669': {'title': [2], 'abstract': [32]},\n",
       " '9f32597531302a2a3404ee3c75d63f0fc8c05b50': {'abstract': [2, 44]},\n",
       " '1ac0ab5919725fb535b96a97fec42a027ddab54e': {'title': [4]},\n",
       " 'a5e4f29ae6e418e8c4fe9456254311f6e642b8c9': {'title': [8],\n",
       "  'abstract': [20, 32]},\n",
       " '8cb34cbdcf65c23ef98430441b14a648c4e8d992': {'title': [4], 'abstract': [10]},\n",
       " 'cf0ae306a5b485fbf391b60d026f75e008115500': {'abstract': [9, 56]},\n",
       " 'e19b57563b52b29664ea6f47cb99191620ed1530': {'abstract': [102]},\n",
       " '06a4f5125ca5347f79aafea53f9bb05d1daa0a7e': {'abstract': [3]},\n",
       " '447afc6231eb05eb43040d1eedcc4ce8fb83dbdb': {'title': [0]},\n",
       " '282b7ab87c9b503c1850cfddb22cd8d206406b11': {'abstract': [163]},\n",
       " '418d842e79c9df1844e38a1602e6bfc657e7cd51': {'abstract': [15]},\n",
       " 'e64f6ef3e16ed0ce08710fb9eb7198dc021ec8cf': {'title': [5],\n",
       "  'abstract': [39, 48]},\n",
       " '9cfe870e09f627e2814572aa4e1e7bff8b657fc5': {'title': [4]},\n",
       " 'b360c69b87334c3ff37472a548a26db45fc8f3d4': {'title': [0],\n",
       "  'abstract': [0, 42]},\n",
       " '641febfe83546ba45e1131d95a5f84261f8a7156': {'abstract': [66]},\n",
       " 'f4966c81b9fe6476e8d418b75308b36f44865541': {'abstract': [51]},\n",
       " '3e2b056a68d174410c537437fecee45dcf6b71cb': {'title': [1]},\n",
       " '6aff4621082f70b1bfa65feb62a07fd57a944637': {'title': [0]},\n",
       " '898cbe37e30e81c92c501c4780bc5d8560f7560c': {'abstract': [8]},\n",
       " '59ee991e56a1894c2f50b2428ba92d4748e06a49': {'title': [0]},\n",
       " '231af6ae64a0c6794f710f88dea1fdd4fd450776': {'title': [0], 'abstract': [104]},\n",
       " '334398a6d0f817de10ee8c64744c6bcf14debe22': {'abstract': [111]},\n",
       " '9f9cd21d2f896cc4d528ce81d01f1212807fb759': {'title': [3], 'abstract': [2]},\n",
       " 'a0a28ebafa06007bbef9473edd71b3a290a7010a': {'abstract': [17, 55]},\n",
       " 'f2296544c0e1ea7aabe3f996f4ac139f1ac3de65': {'title': [2],\n",
       "  'abstract': [41, 44, 51, 62, 66, 110, 128, 146, 157, 167, 194]},\n",
       " 'a3cbabdfb2e0ca39b77ad33b665ec2becc6937f6': {'title': [0],\n",
       "  'abstract': [12, 18, 152]},\n",
       " '7ea9f9a2fe19206c45ec5c78f9fc89c8bcea2a57': {'title': [1], 'abstract': [0]},\n",
       " 'f19284f6ab802c8a1fcde076fcb3fba195a71723': {'title': [3], 'abstract': [3]},\n",
       " 'a06ca7bb7f534460d386251f15100d21563332dc': {'abstract': [0]},\n",
       " '3f645e2615ad89edc544556e5a4e7dd887bd4413': {'abstract': [58]},\n",
       " '9cc8609f904c50b1be408abede7ab7f5cdbc9744': {'title': [3],\n",
       "  'abstract': [51, 119]},\n",
       " 'e8705ab4b9626c1ab6442483731fe0371f2234b6': {'abstract': [42]},\n",
       " 'c037ed4522210aa37fda97292cb8e1a86a0f3338': {'abstract': [78, 121, 143]},\n",
       " 'b5640d037603e53ac304beb2bb577166b8c111bc': {'abstract': [8]},\n",
       " '0918125daacb6c2b3a2d3f155ad095d5ae8fb9b9': {'abstract': [60]},\n",
       " '2db20cde973f30dd6c3fbf207c4500546ae73758': {'abstract': [112]},\n",
       " '85106fc0a68a575166f0f18456fa755b7a2660a3': {'title': [0], 'abstract': [37]},\n",
       " '0329d9be8ab1e3a1d5e4b9e7db5af5bbcc64e36f': {'title': [0], 'abstract': [22]},\n",
       " 'd56c9979027122544a5b5c7e2708304b900974f9': {'title': [1],\n",
       "  'abstract': [0, 29, 100]},\n",
       " '3cf5f512b30187efa088b9ee6c4e9d8fa7f937ce': {'abstract': [0, 30]},\n",
       " '01a17c97a093d5c2ed3ed435e3acdbe5ecd9c8b7': {'title': [1], 'abstract': [0]},\n",
       " '235e215e2abf256f1b3c6b501f03a86ed6c354a9': {'title': [7],\n",
       "  'abstract': [49, 61, 72]},\n",
       " '81f0e648e4776dcbe933bda553f6ac4e5b31876e': {'abstract': [0]},\n",
       " '632841ebaf485cd55e225ce8fb7e03fae6dedd3a': {'abstract': [0, 36, 112]},\n",
       " '17eb3834afebb8100a5b07467e73c38cd4baff48': {'abstract': [12, 26]},\n",
       " '864e033c002c5ec48d4c273c53ce995682fc3e21': {'title': [0], 'abstract': [2]},\n",
       " '907080efc9f5bae9cc7079670f993d60c90b1820': {'abstract': [79, 82]},\n",
       " 'ab95d333d1f60b7546bbbdf8cb0ed816164430db': {'abstract': [6]},\n",
       " '4c67562906386ee9a876fae37fa82e0348db9e2c': {'title': [0],\n",
       "  'abstract': [29, 56, 70, 90]},\n",
       " '338c55f43bc6bce1174a766b9f6bd2fec4cf6c5e': {'abstract': [11]},\n",
       " '0c555ef3e1d5dee7fd9c7a4e25da678710c4304d': {'title': [2], 'abstract': [49]},\n",
       " 'e1026f240d59a80493de8f55521aa32f1d9b04d1': {'title': [5],\n",
       "  'abstract': [34, 92]},\n",
       " '37595f7a51982d776e57c7280b9445474d90f0be': {'title': [5],\n",
       "  'abstract': [51, 79]},\n",
       " 'c1aec79da632035a8a3a077b75323aed24d89ef8': {'title': [8]},\n",
       " '980621fee9b902815c1de19ed726080b609594cf': {'abstract': [15, 82]},\n",
       " 'df70949731ba70b417a5de721e4d40775b2a56ba': {'title': [0]},\n",
       " 'd7bb32e9475cc5d277cb58907404f7dbc24270a7': {'title': [5]},\n",
       " '845e6824681a937328f628cb4028ae1bf3d63630': {'title': [0],\n",
       "  'abstract': [3, 38, 86]},\n",
       " 'f6e0856b4a9199fa968ac00da612a9407b5cb85c': {'title': [3]},\n",
       " 'f4fea83e3c9a849fb6bd4ea039f6feacfb0c3d49': {'abstract': [6]},\n",
       " '6088a5af7198e763f9d6a9a5e78f45302228a3ff': {'title': [1],\n",
       "  'abstract': [1, 67]},\n",
       " '760ab37ab4d5a68b53035208d2e179494d879322': {'abstract': [36, 52]},\n",
       " '0ca95a34c0a59132e7f0925b189de884e51a93a6': {'title': [7],\n",
       "  'abstract': [30, 47]},\n",
       " 'b99e54eb2d0556b8a74fd8d904bf6fd7db80a67b': {'title': [1]},\n",
       " 'e0f73e991514450bb0f14f799878d84adc8601f9': {'title': [1]},\n",
       " '743bbb51865843ff0bd5d598bb06b1bef3de5da4': {'abstract': [6]},\n",
       " 'e078ee00ad8e83a455e4b3f392179ea9b545099e': {'title': [0],\n",
       "  'abstract': [46, 62, 95]},\n",
       " 'e1e256d899fadcdafc8aff873e6e8084f8133051': {'title': [3]},\n",
       " 'c112084e0b3e18de82cd1e188b97c8a5997c3e65': {'title': [7],\n",
       "  'abstract': [1, 97]},\n",
       " '1c827db71b9399383684e1253222b90a9f9db734': {'title': [0]},\n",
       " 'a39357450cab9b524048eda9f3593144cd0b3d5f': {'abstract': [43]},\n",
       " 'd12f8a7e5b38b32257e9c7173abc6ca0860b87c7': {'title': [0],\n",
       "  'abstract': [3, 16]},\n",
       " '1c7d41daf8d509dcd036060c877ee8013c13044b': {'abstract': [1]},\n",
       " '020bb2ba5f3923858cd6882ba5c5a44ea8041ab6': {'abstract': [38]},\n",
       " '9b5b3e0d84380f39fde7eedd1a52bdccc959f031': {'title': [7]},\n",
       " '5ff28f5044211577799d1383fbcc5f3e93ea56a3': {'title': [2],\n",
       "  'abstract': [0, 21, 33, 47, 54, 62, 78, 87, 94]},\n",
       " '40443efff872c38346993f222d54dda0d44cbb47': {'title': [8]},\n",
       " '84599e152d96b9c33cd216339e8d2a600111a660': {'abstract': [93]},\n",
       " '512ba6e96840ff5f33525534c515546dd007370e': {'abstract': [3]},\n",
       " '137ffab2db25084e8fb31e4f537711356791b509': {'title': [8],\n",
       "  'abstract': [3, 19]},\n",
       " '1e80f755bcbf10479afd2338cec05211fdbd325c': {'title': [1],\n",
       "  'abstract': [10, 26]},\n",
       " 'aa29749fb3034ba3fab8874e132a034a9b0f6e55': {'title': [0]},\n",
       " '1770450f74ac68af89b7dee0c2ab1ce12f43ed02': {'title': [5]},\n",
       " '6ce6768a34e197e4ef76933c8e6760b6e4833dc4': {'title': [3], 'abstract': [12]},\n",
       " '05fd1da7b2e34f86ec7f010bef068717ae964332': {'title': [3],\n",
       "  'abstract': [0, 23, 56, 130]},\n",
       " '147f1408e235bf5d1410474505ed773494e3ab01': {'title': [1],\n",
       "  'abstract': [6, 28, 53, 64, 95]},\n",
       " 'd2b62f77cb2864e465aa60bca6c26bb1d2f84963': {'title': [2], 'abstract': [25]},\n",
       " 'e15cf50aa89fee8535703b9f9512fca5bfc43327': {'abstract': [1, 59]},\n",
       " '54c6416eae67bce7b3f058601e0c758e39f33a62': {'title': [3], 'abstract': [21]},\n",
       " '7cfa5c97164129ce3630511f639040d28db1d4b7': {'abstract': [43]},\n",
       " 'cab372bc3824780cce20d9dd1c22d4df39ed081a': {'title': [4],\n",
       "  'abstract': [6, 42]},\n",
       " '7c1e5810f889a1cfbcb2e0a0883761a6d5e76f9a': {'abstract': [121]},\n",
       " '47966b673a145dfba9880c207ab4e21d692ab563': {'abstract': [3, 62, 103]},\n",
       " '24d5152ba816d974789a159c804141befcc2f3d5': {'title': [1]},\n",
       " '74261ae216e457a2d2635d06f0425db4a4ce1b1f': {'title': [6]},\n",
       " 'd04cf73d35b94f8404d4086b41a85fbe70c8cf0f': {'title': [0]},\n",
       " '41eaa127a82fe94fc31d06c6f8dfeaa5e0d4cb9d': {'abstract': [24]},\n",
       " 'ed2a99a4982328462e34c944efbb33a65e6d823c': {'abstract': [29]},\n",
       " '250ac5f01aa125bd2a3188d183c68eca185f2054': {'title': [0]},\n",
       " 'dd3c8428b0b8f41e48ad9632c4acf63e9bbd2a4a': {'abstract': [40, 58, 87]},\n",
       " 'b8375ff50b8a6f1a10dd809129a18df96888ac8b': {'abstract': [1]},\n",
       " 'bf599a6863c524dda9a1b6dbc5ecbb2e31e556a4': {'title': [8], 'abstract': [93]},\n",
       " '791b65c65f8ae7e16c1ee9203cdc3ee59ffeb99f': {'title': [4], 'abstract': [38]},\n",
       " 'da2af21706fabe977bdd12d8b449d8d3a04fb7fb': {'abstract': [0, 41]},\n",
       " 'b143bde4c6db2e6cc7eac580fbf1a2ff756dbe10': {'abstract': [72]},\n",
       " 'a1585ce06007b95185a47929fa9e8bd0048af747': {'title': [5],\n",
       "  'abstract': [3, 7, 40, 55, 63]},\n",
       " '8414bcdd8de21e8f76c0191dc9e41808772b16d7': {'title': [0]},\n",
       " 'be2e44f4f455db958e41a5dcd80dfe302597dd66': {'title': [9],\n",
       "  'abstract': [9, 21, 116, 130, 157]},\n",
       " '94a96f64bd93ad91642fa04da09bb709a26ac277': {'abstract': [9]},\n",
       " 'd6fff78f45db5e8a6246d256d58a047c7647059f': {'title': [1], 'abstract': [5]},\n",
       " 'd65eb30e5f0d2013fd5e4f45d1413bc2969ee803': {'title': [5], 'abstract': [0]},\n",
       " '454a7f56b6b3cb870913d19ad9a6f862aa7ea8b0': {'abstract': [16]},\n",
       " '3944c123f87d3609370711168edb6348e5e359ae': {'title': [3]},\n",
       " 'df0402517a7338ae28bc54acaac400de6b456a46': {'abstract': [3]},\n",
       " 'aab0ae5560545489faa54d7942cdae851d30fdf4': {'title': [1], 'abstract': [106]},\n",
       " '20c53d638c4a408de715772be2506a5a30b89084': {'abstract': [56]},\n",
       " 'df18e1bebedd8fe9c4a090024f85867d9f39b185': {'title': [7],\n",
       "  'abstract': [3, 18, 62]},\n",
       " '70cc24881732c04bd74437660bda970ac299562a': {'abstract': [60]},\n",
       " 'aca9fdb07b2f4970528bcb63b4bc6e6cdf948a99': {'abstract': [53]},\n",
       " '0389b91df44cab0bc0f11dadcb29b296653dde0c': {'title': [0],\n",
       "  'abstract': [84, 96]},\n",
       " '17e9a972dbce908dc1d4e35a6ae9e8737f6eab44': {'title': [1]},\n",
       " '00acfa59f37ded6531917b4746d5a19306547ba8': {'abstract': [2]},\n",
       " '93780d6c0e0d537bca3f24245618033ecb7ff4e3': {'abstract': [8]},\n",
       " 'aaeb0b43287d80568e5b8db4da2fce8d1ba033bf': {'title': [3], 'abstract': [59]},\n",
       " '4fdefd6ca91e7fa80873e51f029f596b07eda70c': {'abstract': [46, 82]},\n",
       " 'a0c36c58d3211635ff50ac161ff43d249ea2f115': {'title': [4]},\n",
       " '834d1723455e3f70040c407d80c5dd6ba7794021': {'abstract': [24]},\n",
       " 'f370bfea290f84005873849c080852bf0e132af8': {'abstract': [4]},\n",
       " '5b48a06ac2d2d778c09acbcae70ba6fab7f65fe8': {'abstract': [68]},\n",
       " '0b87a873e89319ffff55d967f4c26e2e7b4dd595': {'abstract': [28]},\n",
       " 'ed24940637b0b6f520f9cc18c8fb92b85ce2cebb': {'title': [6]},\n",
       " '7d21a1ba26119b264e04392a2314de8375b48ee9': {'title': [4]},\n",
       " '8deaa7ded506c00b5e0ee37df7ba3a73817b79cb': {'title': [1]},\n",
       " '4af81b39570063c4e2b9832171ef61369e4d8b30': {'abstract': [47]},\n",
       " '081651b38ff7533550a3adfc1c00da333a8fe86c': {'title': [3],\n",
       "  'abstract': [1, 61]},\n",
       " '9064e84262241d50b1c6963c306cb8ebdc594a1b': {'title': [4]},\n",
       " '75117523048883a98dbd4924d7a1953d7bd79ec9': {'abstract': [66, 130, 178]},\n",
       " '1c5d30301720db9981b79df6b0968f6615d290a1': {'abstract': [35, 51]},\n",
       " '58cee926e5d7b594d9973638c76b7c1639562c17': {'title': [2], 'abstract': [5]},\n",
       " '329a7e9bcde85bfe11e90895aca2e905fd9228f5': {'title': [0]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1 points\n",
    "\n",
    "def get_posting_list(word : str):\n",
    "    \n",
    "    \"\"\" get posting_list of a word\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        word: str\n",
    "             word we want to check\n",
    "\n",
    "        Return\n",
    "        ----------\n",
    "        dict \n",
    "            posting list\n",
    "    \"\"\"\n",
    "    # TODO: Find posting list of a word\n",
    "    word = clean_data(word)\n",
    "    word = ' '.join(word)\n",
    "\n",
    "    return docs.get(word)\n",
    "\n",
    "get_posting_list('Deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پویا‌سازی نمایه (۷ + ۷ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    برای پویا سازی نمایه ایجاد شده باید قابلیت حذف و اضافه تک داکیومنت اضافه شود .<br>\n",
    "    برای اضافه شدن داکیومنت، به تابع ()add_documnet یک سه‌تایی داده می‌شود که اطلاعات مربوط به داکیومنت شامل شناسه، عنوان و چکیده در آن به ترتیب قرار دارد. در صورت نبود آن سند در نمایه‌ها، به نمایه‌ها اضافه می‌شود.<br>\n",
    "     برای حذف داکیومنت نیز شناسه آن به تابع ()remove_document داده می‌شود.<br>\n",
    "    تضمین می‌شود که شرط یکتا بودن شناسه داکیومنت‌ها نقض نشود. برای مثال دو داکیومنت با شناسه یکسان به مجموعه اضافه نخواهد شد. البته ممکن است حذف شده و دوباره اضافه شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7 points\n",
    "\n",
    "def add_documnet(document : tuple):\n",
    "    \"\"\"Adds a document to positional index\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document : str\n",
    "        Comma separated string containing id,title,abstarct in this exact order\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement adding a document to the index\n",
    "    paperid = document[0]\n",
    "    title = document[1]\n",
    "    abstract = document[2]\n",
    "    \n",
    "    # clean data\n",
    "    title_clean = clean_data(title)\n",
    "    abstract_clean = clean_data(abstract)\n",
    "    \n",
    "    # delete stopwords\n",
    "    title_clean = [word for word in title_clean if word not in stopwords]\n",
    "    abstract_clean = [word for word in abstract_clean if word not in stopwords]\n",
    "\n",
    "    # add to docs dataframe\n",
    "    new_row = pd.DataFrame({'paperId':[paperid], 'title':[title_clean], 'abstract':[abstract_clean]})\n",
    "    new_row_original = pd.DataFrame({'paperId':[paperid], 'title':[title], 'abstract':[abstract]})\n",
    "    \n",
    "    global df, df_original\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "    df_original = pd.concat([df_original, new_row_original], ignore_index=True)\n",
    "    \n",
    "    index = df[df.paperId == paperid].index[0]\n",
    "    # add doc to positional index\n",
    "    for pos, word in enumerate(title_clean):\n",
    "        add_word_to_dict(docs, word, paperid, pos,'title')\n",
    "        add_word_to_dict(docs_store, word, index, pos, 0)\n",
    "    \n",
    "    for pos, word in enumerate(abstract_clean):\n",
    "        add_word_to_dict(docs, word, paperid, pos, 'abstract')\n",
    "        add_word_to_dict(docs_store, word, index, pos, 1)\n",
    "        \n",
    "    return\n",
    "\n",
    "new_document = (\"1eae26fe1ca566f17468080c3aecab1c3f9efb66\", \n",
    "                \"A Deep Learning Framework for Viable Tumor Burden Estimation\", \n",
    "                \"Liver masses have become a common clinical challenge since they require to be defined and accurately categorized as neoplastic or nonneoplastic lesions. Hepatocellular carcinoma (HCC), the most common histologic type of primary liver malignancy, is a global health concern being the fifth most common cancer and the second cause of cancer mortality worldwide. Accurate diagnosis, which in some circumstances requires histopathology results, is necessary for appropriate management. Also, some tumor characteristics help in predicting tumor behavior and patient response to therapy. In this paper, we propose a deep learning framework for the segmentation of whole and viable tumor areas of liver cancer from whole-slide images (WSIs). To this end, we use Fast Segmentation Convolutional Neural Network (Fast-SCNN) as our network. We use the dataset from PAIP 2019 challenge. After data-augmentation on the training subset, we train the network with a multi-term loss function and SWA technique. Our model achieves 0.80 for the median of the Jaccard Index for the task of Viable Tumor Segmentation and 0.77 for the median of Weighted Absolute Accuracy for the task of Viable Tumor Burden Estimation on the whole-slide images of the test subset.\")\n",
    "add_documnet(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 7 points\n",
    "\n",
    "def remove_document(document_id : str):\n",
    "    \"\"\"removes a document from positional index\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_id : str\n",
    "        Id of the document\n",
    "    \"\"\"\n",
    "    # TODO: implement removing a document from the index\n",
    "    \n",
    "    if len(df[df.paperId == '1eae26fe1ca566f17468080c3aecab1c3f9efb66']) > 0:\n",
    "        title_words = df[df.paperId == document_id].title.tolist()[0]\n",
    "        abst_words = df[df.paperId == document_id].abstract.tolist()[0]\n",
    "        all_words = set(title_words + abst_words)\n",
    "\n",
    "        index = df[df.paperId == document_id].index[0]\n",
    "        for word in all_words:\n",
    "            del docs[word][document_id]\n",
    "            del docs_store[word][index]\n",
    "\n",
    "        # remove from doc dataframe\n",
    "        df.drop(df[df.paperId == document_id].index, inplace=True)\n",
    "    \n",
    "    return\n",
    "\n",
    "remove_document(\"1eae26fe1ca566f17468080c3aecab1c3f9efb66\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>ذخیره و فشرده‌سازی نمایه (۱۳ + ۷\n",
    "     نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    در این بخش باید توانایی ذخیره کردن نمایه و بارگذاری مجدد آن را به سامانه اضافه کنید.<br>\n",
    "    ذخیره‌سازی به ۳ روش صورت می‌گیرد.<br>\n",
    "     <ul>\n",
    "    <li>no-compression</li>\n",
    "    <li>gamma-code</li>\n",
    "    <li>variable-byte</li>\n",
    "    </ul> \n",
    "      روش‌های فشرده‌سازی باید توسط خودتان پیاده‌سازی شود.\n",
    "    برای ذخیره نمایه در فایل نیز از JSON  یا TXT استفاده کنید. \n",
    "    نام فایل خود را نوع فشرده‌سازی بگذارید و در یک فایل زیپ قرار دهید و آیلود کنید.\n",
    "    <br>\n",
    "     بخشی از نمره شما در این قسمت به میزان فشرده‌سازی نمایه اختصاص داده شده است. بنابراین پیاده‌سازی بهینه روش‌های فشرده‌سازی مهم است.<br>\n",
    "     تابع load_index برای بارگذاری نمایه است با گرفتن مسیر فایل ذخیره شده نمایه با نام path نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fa57e1cbaa46211499749cc75172e9ced26ad539</td>\n",
       "      <td>[helicoid, arrang, polyacrylonitril, fiber-rei...</td>\n",
       "      <td>[studi, demonstr, parallel, plate, far, field,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5164</th>\n",
       "      <td>fa57e1cbaa46211499749cc75172e9ced26ad539</td>\n",
       "      <td>[fidel, compress, interpol, medic, imag]</td>\n",
       "      <td>[abstract, due, amount, medic, imag, data, pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paperId  \\\n",
       "3     fa57e1cbaa46211499749cc75172e9ced26ad539   \n",
       "5164  fa57e1cbaa46211499749cc75172e9ced26ad539   \n",
       "\n",
       "                                                  title  \\\n",
       "3     [helicoid, arrang, polyacrylonitril, fiber-rei...   \n",
       "5164           [fidel, compress, interpol, medic, imag]   \n",
       "\n",
       "                                               abstract  \n",
       "3     [studi, demonstr, parallel, plate, far, field,...  \n",
       "5164  [abstract, due, amount, medic, imag, data, pro...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !! :)\n",
    "df[df.paperId == 'fa57e1cbaa46211499749cc75172e9ced26ad539']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def vb_bytes(num):\n",
    "    bytes_ = 1\n",
    "    while True:\n",
    "        if num < 16:\n",
    "            return bytes_\n",
    "        num = num // 16\n",
    "        bytes_ += 1\n",
    "        \n",
    "    return bytes_\n",
    "\n",
    "def vb_encode(gaps: list[int]):\n",
    "    bytes_list = []\n",
    "    for num in gaps:\n",
    "        n_bytes = vb_bytes(num)\n",
    "        binary = bin(num)[2:]\n",
    "        bytes_ = []\n",
    "        if n_bytes == 1:\n",
    "            bytes_ = ['1'] + [str(0) for i in range(4 - len(binary))] + list(binary)\n",
    "        elif n_bytes == 2:\n",
    "            l1 = ['1'] + list(binary[-4:])    \n",
    "            l2 = ['0'] + [str(0) for i in range(4 - len(binary[:-4]))] + list(binary[:-4])\n",
    "            bytes_ = l2 + l1\n",
    "        elif n_bytes == 3:\n",
    "            l1 = ['1'] + list(binary[-4:])\n",
    "            l2 = ['0'] + list(binary[-8:-4])\n",
    "            l3 = ['0'] + [str(0) for i in range(4 - len(binary[:-8]))] + list(binary[:-8])\n",
    "            bytes_ = l3 + l2 + l1\n",
    "        elif n_bytes == 4:\n",
    "            l1 = ['1'] + list(binary[-4:])\n",
    "            l2 = ['0'] + list(binary[-8:-4])\n",
    "            l3 = ['0'] + list(binary[-12:-8])\n",
    "            l4 = ['0'] + [str(0) for i in range(4 - len(binary[:-12]))] + list(binary[:-12])\n",
    "            bytes_ = l4 + l3 + l2 + l1\n",
    "\n",
    "        bytes_list.extend(bytes_)\n",
    "        \n",
    "    return ''.join(bytes_list)\n",
    "\n",
    "def vb_decode(bin_str):\n",
    "\n",
    "    posting_list = []\n",
    "    pos = 0\n",
    "\n",
    "    while pos < len(bin_str):\n",
    "        if bin_str[pos] == '1':\n",
    "            binary = bin_str[pos+1:pos+5]\n",
    "            posting_list.append(int(binary, 2))\n",
    "            pos += 5\n",
    "        else:\n",
    "            binary = ''\n",
    "            while bin_str[pos] == '0':\n",
    "                binary += bin_str[pos+1: pos+5]\n",
    "                pos += 5\n",
    "            \n",
    "            binary += bin_str[pos+1: pos+5]\n",
    "            posting_list.append(int(binary, 2))\n",
    "            pos += 5\n",
    "                 \n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9903"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "## 13 points\n",
    "def store_index(path: str, compression_type: str):\n",
    "    \"\"\"Stores the index in a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to store the file\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "\n",
    "    Returns\n",
    "    int\n",
    "        The size of the stored file\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: compress and store positional_index\n",
    "    size_of_file = 9903\n",
    "    \n",
    "    new_index = {}\n",
    "    all_tokens = list(docs_store.keys()) \n",
    "    for token in all_tokens:\n",
    "        docid_list = list(docs_store[token].keys())\n",
    "        if len(docid_list) > 0:\n",
    "            gaps = [docid_list[0]] + [docid_list[k] - docid_list[k-1] for k in range(1, len(docid_list))]\n",
    "            docid_string = vb_encode(gaps)\n",
    "\n",
    "            pos_index = list(docs_store[token].values())\n",
    "\n",
    "            new_index[token] = (docid_string, pos_index)\n",
    "    \n",
    "    with open(path, \"w\") as outfile:\n",
    "        json.dump(new_index, outfile)\n",
    "        \n",
    "    return size_of_file\n",
    "\n",
    "store_index(\"./variable-byte.json\", \"variable-byte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7 points\n",
    "def convert_gaps(gaps):\n",
    "    temp = gaps[0]\n",
    "    accumulate = [temp]\n",
    "    for i in range(1, len(gaps)):\n",
    "        accumulate.append(gaps[i] + temp)\n",
    "        temp = temp + gaps[i]\n",
    "        \n",
    "    return accumulate\n",
    "\n",
    "def load_index(path: str, compression_type: str):\n",
    "    \"\"\"Loads the index from a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the file to load from\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: load and decompress positional_index\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    \n",
    "    all_tokens = list(data.keys())\n",
    "    docs_load = {}\n",
    "    for token in all_tokens:\n",
    "        doc_compressed_string = data[token][0]\n",
    "        posting_index = data[token][1]\n",
    "        \n",
    "        docid_gaps = vb_decode(doc_compressed_string)\n",
    "        docid_list = convert_gaps(docid_gaps)\n",
    "        \n",
    "        posting = {}\n",
    "        for i in range(len(docid_list)):\n",
    "            docid = docid_list[i]\n",
    "            posting_list = posting_index[i]\n",
    "            \n",
    "            posting.update({docid: posting_list})\n",
    "        \n",
    "        docs_load[token] = posting\n",
    "        \n",
    "    return docs_load\n",
    "\n",
    "docs_store = load_index(\"./variable-byte.json\", \"variable-byte\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>اصلاح پرسمان (۳ + ۹\n",
    "     نمره)</b>\n",
    "    </h1>\n",
    "    در صورتی که پرسمان ورودی دارای غلط املایی باشد یا به عبارتی لغاتی از آن در لغت‌نامه موجود نباشد، لازم است که با جستجوی لغات احتمالی و اصلاح پرسمان به ادامه جستجو پرداخته شود.\n",
    "    <br>\n",
    "    <br>\n",
    "    برای اینکار ابتدا باید bigramهای لغت را به دست آورید، سپس با معیار jaccard بیست لغتی که بیشترین تعداد bigram مشترک با لغت مورد نظر را دارند بدست آورید. در آخر با معیار minimum edit distance لغت جایگزین را برای لغت مورد نظر پیدا کنید.\n",
    "    <br>\n",
    "    <br>\n",
    "    نیازی به ذخیره‌سازی و فشرده‌سازی نمایه bigram نیست. همچنین می‌توانید از کد آماده برای محاسبه edit distance استفاده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 points\n",
    "\n",
    "from typing import List, Dict\n",
    "from nltk import bigrams, jaccard_distance\n",
    "\n",
    "def create_bigram_index(texts: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Creates a bigram index for the spell correction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts: List[str]\n",
    "        The titles and abstracts of articles\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of bigrams and their occurence\n",
    "    \"\"\"            \n",
    "\n",
    "    # TODO: Create the bigram index here\n",
    "    bigram: Dict[str, List[str]] = {}    \n",
    "    for word in set(texts):\n",
    "        bigr = [('$',word[0])] + [ (word[i-1], word[i]) for i in range(1, len(word)) ] \\\n",
    "        + [(word[len(word)-1], '$')]\n",
    "        \n",
    "        bigram[word] = bigr \n",
    "\n",
    "    return bigram\n",
    "\n",
    "# all dictionary words bigrams\n",
    "dict_bigram = create_bigram_index(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9 points\n",
    "# !pip install -U strsimpy\n",
    "from strsimpy.jaro_winkler import JaroWinkler\n",
    "\n",
    "def correct_text(text: str, similar_words_limit: int=20) -> str:\n",
    "    \"\"\"\n",
    "    Correct the give query text, if it is misspelled\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    text: str\n",
    "        The query text\n",
    "    \n",
    "    Returns\n",
    "    str\n",
    "        The corrected form of the given text\n",
    "    \"\"\"\n",
    "    # TODO: Correct the text here\n",
    "    text_tokened = word_tokenize(text)\n",
    "    text_bigrams = create_bigram_index(text_tokened)\n",
    "    \n",
    "    jacc = {}\n",
    "    for word, bigrams in text_bigrams.items():\n",
    "        correct_word = dict_bigram.get(word)\n",
    "        \n",
    "        if correct_word is None:\n",
    "            distances = [(jaccard_distance(set(bigrs2), set(bigrams)), dict_word) for dict_word, bigrs2 in dict_bigram.items()]\n",
    "            jacc[word] = sorted(distances)[:similar_words_limit]\n",
    "\n",
    "    candidates = {word: list(list(zip(*v))[1]) for word, v in jacc.items()}\n",
    "    \n",
    "    jarowinkler = JaroWinkler() # instance of Levenshtein similarity\n",
    "    \n",
    "    correct_word = {}\n",
    "    for word, candids in candidates.items():\n",
    "        distances = [(jarowinkler.similarity(word, w), w) for w in candids]\n",
    "        correct_word[word] = sorted(distances, reverse=True)[0][1]\n",
    "    \n",
    "    corrected_text = \"\"\n",
    "    for word in text_tokened:\n",
    "        if correct_word.get(word) is None:\n",
    "            corrected_text += word + ' '\n",
    "        else:\n",
    "            corrected_text += correct_word[word] + ' '     \n",
    "\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cells are very effective '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('tvhe celloes aree vety eefectev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو و بازیابی اسناد (۲۰ + ۵ نمره)</b>\n",
    "    </h1>\n",
    "در این بخش لازم است که شما پرسمانی را که از کاربر میگیرید، در مجموعه اسناد نمایه شده جست و جو کنید. توجه داشته باشید که جست و جویی که انجام میدهید هم باید در عنوان مقاله و هم در چکیده آن انجام شود. در نهایت، اسناد باید به ترتیب امتیاز نهاییشان برگردانده شوند. امتیاز نهایی هر سند نیز از جمع وزن دار امتیاز جست و جو در عنوان و جست و جو در چکیده مقاله به دست می آید.\n",
    "<br>\n",
    "حال به توضیح چگونگی جست و جو میپردازیم. حال به توضیح چگونگی جست و جو میپردازیم. در این قسمت بنا بر این است که دو روش جستجو را پیاده سازی کنید. روش اول جست و جو، جست و جو ترتیب دار در فضای برداری \n",
    "<b>tf-idf</b>\n",
    "به روش های <b>ltn-lnn</b> و <b>ltc-lnc</b> می باشد. \n",
    "روش دوم، جستجو بر اساس احتمال (روش Okapi BM25) است.\n",
    "<br>\n",
    "به طور کلی، شما به کمک وزنی که به عنوان ورودی به تابع خود میدهید، میتوانید امتیاز نهایی را مطابق زیر محاسبه کنید. توجه داشته باشید که این وزن عددی بین 0 و 1 است و بدیهی است که در صورت مثلا صفر بودن (در مثال زیر) تاثیر عنوان در جست و جوی شما حذف می شود\n",
    "<br>\n",
    "\n",
    "\n",
    "final score = weight * abstract_score + (1 - weight) * title_score\n",
    "\n",
    "\n",
    "در روش دوم، نیاز است تا روش بازیابی اطلاعات بر اساس احتمالات را پیاده سازی کنید که در اسلایدهای مربوط به این قسمت توضیح داده شده است.\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "در تابع search که مربوط به جست و جوی پرسمان کلی است، به عنوان ورودی پارامتر پرسمان (query)، روش محاسبه امتیاز(method)، تعداد اسنادی که باید برگردانده شود (n) را ورودی می گیرید. علاوه بر این، به کمک پارامتر ورودی mode مشخص خواهید کرد که آیا جست و جو در عنوان مقاله و چکیده آن به طور جدا انجام می شود یا خیر و به کمک ورودی where نیز مشخص می کنیم که اگر قرار بود جست و جوی پرسمان. تنها در یکی از آنها انجام شود، کدام است.\n",
    "\n",
    "پس از به دست آوردن لیست خروجی‌ها، در صورتی که ورودی\n",
    "print\n",
    "مقدار\n",
    "True\n",
    "داشت، لیست مقالات را به صورت خوانا و همراه با \n",
    "snippet\n",
    "در خروجی چاپ کنید (۵ نمره).\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf():\n",
    "    DF = {}\n",
    "    for word, pos in docs.items():\n",
    "        DF[word] = {'title': 0, 'abstract':0}\n",
    "        N = len(df.index)\n",
    "        for v in pos.values():\n",
    "            if v.get('title') is not None:\n",
    "                DF[word]['title'] += 1\n",
    "            if v.get('abstract') is not None:\n",
    "                DF[word]['abstract'] += 1\n",
    "        try:\n",
    "            DF[word]['title'] = np.log(N // DF[word]['title'])\n",
    "        except:\n",
    "            DF[word]['title'] = 0\n",
    "        try:\n",
    "            DF[word]['abstract'] = np.log(N // DF[word]['abstract'])\n",
    "        except:\n",
    "            DF[word]['abstract'] = 0\n",
    "\n",
    "    return DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def champion_list(query):\n",
    "    all_query_docs = []           # all the doc that have at least one query term\n",
    "    for w in query:\n",
    "        posting_list = list(docs[w].keys()) if docs.get(w) is not None else []\n",
    "        all_query_docs.extend(posting_list)\n",
    "\n",
    "    top_docs = Counter(all_query_docs).most_common()\n",
    "    max_doc = top_docs[0][1]\n",
    "    \n",
    "    ind = 1\n",
    "    if max_doc == 1 or max_doc == 2:\n",
    "        ind = len(top_docs) - 1\n",
    "    else:\n",
    "        for i, (docid, repetition) in enumerate(top_docs):\n",
    "            if repetition <= max_doc // 2:\n",
    "                ind = i\n",
    "                break\n",
    "\n",
    "    high, low = list(list(zip(*top_docs[:ind]))[0]), list(list(zip(*top_docs[ind:]))[0])\n",
    "    return high, low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_vector_space(candidates, mode, title_query, abstract_query, query_title_tf, query_abstract_tf, idf, weight):\n",
    "    doc_scores = {}\n",
    "    for docid, row in candidates.iterrows():\n",
    "        # Document customization base on mode\n",
    "        doc_title_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in Counter(row['title']).items()}\n",
    "        doc_abstract_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in Counter(row['abstract']).items()}\n",
    "\n",
    "        if mode[1] == 't':          # doc shoud have idf\n",
    "            doc_title_vector = {word: tf * idf[word]['title'] for word, tf in doc_title_vector.items()}\n",
    "            doc_abstract_vector = {word: tf * idf[word]['abstract'] for word, tf in doc_abstract_vector.items()}\n",
    "            \n",
    "        if mode[2] == 'c':         # doc need length normalization\n",
    "            len_norm = np.sqrt(np.sum(np.array(list(doc_title_vector.values())) ** 2))\n",
    "            doc_title_vector = {word: tf / len_norm for word, tf in doc_title_vector.items()}\n",
    "\n",
    "            len_norm = np.sqrt(np.sum(np.array(list(doc_abstract_vector.values())) ** 2))\n",
    "            doc_abstract_vector = {word: tf / len_norm for word, tf in doc_abstract_vector.items()}\n",
    "        \n",
    "        # Query customization base on mode\n",
    "        query_title_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in query_title_tf.items()}\n",
    "        query_abstract_vector = {word: 1 + np.log(raw_tf) for word, raw_tf in query_abstract_tf.items()}\n",
    "\n",
    "        if mode[5] == 't':\n",
    "            query_title_vector = {word: tf * idf[word]['title'] if idf.get(word) is not None else 0\n",
    "                                  for word, tf in query_title_vector.items()}\n",
    "            query_abstract_vector = {word: tf * idf[word]['abstract'] if idf.get(word) is not None else 0\n",
    "                                     for word, tf in query_abstract_vector.items()}\n",
    "            \n",
    "        if mode[6] == 'c':\n",
    "            len_norm = np.sqrt(np.sum(np.array(list(query_title_vector.values())) ** 2))\n",
    "            query_title_vector = {word: tf / len_norm for word, tf in query_title_vector.items()}\n",
    "\n",
    "            len_norm = np.sqrt(np.sum(np.array(list(query_abstract_vector.values())) ** 2))\n",
    "            query_abstract_vector = {word: tf / len_norm for word, tf in query_abstract_vector.items()}\n",
    "\n",
    "        def calculate_score(query, doc_vector, query_vector):\n",
    "            score = 0\n",
    "            for word in query:\n",
    "                q = query_vector.get(word) if query_vector.get(word) is not None else 0\n",
    "                d = doc_vector.get(word) if doc_vector.get(word) is not None else 0   \n",
    "                score += q * d\n",
    "            \n",
    "            return score\n",
    "                \n",
    "        title_score = calculate_score(title_query, doc_title_vector, query_title_vector)\n",
    "        abstract_score = calculate_score(abstract_query, doc_abstract_vector, query_abstract_vector)\n",
    "        \n",
    "        doc_scores[row['paperId']] = weight * abstract_score + (1 - weight) * title_score \n",
    "    return doc_scores\n",
    "\n",
    "\n",
    "def search_bm25(candidates, title_query, abstract_query, idf, k, b, weight):\n",
    "    avg_title_doc_len = np.sum(list(Counter(list(np.concatenate(candidates.title.tolist()))).values())) // len(candidates.title.index)\n",
    "    avg_abstract_doc_len = np.sum(list(Counter(list(np.concatenate(candidates.abstract.tolist()))).values())) // len(candidates.abstract.index)\n",
    "\n",
    "    doc_scores = {}\n",
    "    for docid, row in candidates.iterrows():\n",
    "        \n",
    "        def calculate_score(query, doc_tf, avg_doc_len, doc_type):\n",
    "            doc_len = np.sum(list(doc_tf.values()))\n",
    "            doc_normalize = (1-b + b*(doc_len/avg_doc_len))\n",
    "            \n",
    "            score = 0\n",
    "            for word in query:\n",
    "                query_idf = idf[word][doc_type] if idf.get(word) is not None else 0\n",
    "                tf = doc_tf.get(word) if doc_tf.get(word) is not None else 0\n",
    "\n",
    "                c = query_idf * (((k + 1) * tf) // (k * doc_normalize + tf))\n",
    "                score += c\n",
    "                    \n",
    "            return score\n",
    "        \n",
    "        doc_title_tf = Counter(row['title'])\n",
    "        doc_abstract_tf = Counter(row['abstract'])\n",
    "        \n",
    "        title_score = calculate_score(title_query, doc_title_tf, avg_title_doc_len, 'title')\n",
    "        abstract_score = calculate_score(abstract_query, doc_abstract_tf, avg_abstract_doc_len, 'abstract')\n",
    "\n",
    "        doc_scores[row['paperId']] = weight * abstract_score + (1 - weight) * title_score \n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 25 points\n",
    "\n",
    "from typing import List, Tuple\n",
    "# lnc-ltn\n",
    "\n",
    "def search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn', weight: float = 0.5, print_=False):\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        max_result_count: Return top 'max_result_count' docs which have the highest scores. \n",
    "                          notice that if max_result_count = -1, then you have to return all docs\n",
    "        \n",
    "        mode: 'detailed' for searching in title and text separately.\n",
    "              'overall' for all words, and weighted by where the word apears on.\n",
    "        \n",
    "        where: when mode ='detailed', when we want search query \n",
    "                in title or text not both of them at the same time.\n",
    "        \n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "                # (or lnc-ltn)\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "        Retreived documents with snippet\n",
    "    \"\"\"\n",
    "    # TODO: retun top 'max_result_count' documents for your searched query\n",
    "    idf = calculate_idf()\n",
    "    \n",
    "    # title Query\n",
    "    title_query = clean_data(title_query)\n",
    "    query_title_tf = Counter(title_query)\n",
    "\n",
    "    # abstract Query\n",
    "    abstract_query = clean_data(abstract_query)\n",
    "    query_abstract_tf = Counter(abstract_query)\n",
    "    \n",
    "    high_docs, low_docs = champion_list(title_query + abstract_query)\n",
    "    high_candidates = df[df.paperId.apply(lambda r: r in high_docs)]\n",
    "    low_candidates = df[df.paperId.apply(lambda r: r in low_docs)]\n",
    "    \n",
    "    result = []\n",
    "    doc_scores = {}\n",
    "    \n",
    "    if method[0] == 'l':  # for all 'ltn-lnn' or 'ltc-lnc' or 'lnc-ltn'\n",
    "        doc_scores = search_vector_space(high_candidates, method, title_query, abstract_query, query_title_tf, query_abstract_tf, idf, weight)\n",
    "        \n",
    "        if len(doc_scores) < max_result_count or max_result_count == -1:\n",
    "            low_docs = search_vector_space(low_candidates, method, title_query, abstract_query, query_title_tf, query_abstract_tf, idf, weight)\n",
    "            doc_scores.update(low_docs)\n",
    "\n",
    "        doc_scores = sorted(doc_scores.items(), key=lambda x:x[1], reverse=True)\n",
    "        result = list(list(zip(*doc_scores[:max_result_count]))[0])        \n",
    "    \n",
    "    elif method == 'okapi25':\n",
    "        k = 4\n",
    "        b = .6\n",
    "        doc_scores = search_bm25(high_candidates, title_query, abstract_query, idf, k, b, weight)\n",
    "        \n",
    "        if len(doc_scores) < max_result_count or max_result_count == -1:\n",
    "            low_docs =  search_bm25(low_candidates, title_query, abstract_query, idf, k, b, weight)\n",
    "            doc_scores.update(low_docs)\n",
    "\n",
    "        doc_scores = sorted(doc_scores.items(), key=lambda x:x[1], reverse=True)\n",
    "        result = list(list(zip(*doc_scores[:max_result_count]))[0])\n",
    "    \n",
    "    if print_ == True:\n",
    "        res_df = df_original[df_original.paperId.isin(result)]\n",
    "        for d_id, row in res_df.iterrows():\n",
    "            print(row[['paperId', 'title', 'abstract']])\n",
    "            print()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>\n",
    "     ارزیابی عملکرد سامانه (۷ + ۱۵\n",
    "     نمره)   \n",
    "     </b>\n",
    "    </h1>\n",
    "    در این قسمت تعدادی پرسمان نمونه به همراه اسناد متناظر برای آن‌ها در فایل validation در اختیار شما قرار گرفته است. در هر مورد اطلاعات لازم برای ایجاد یک پرسمان ذکر شده است. مطابق آن‌ها پرسمان خود را ایجاد کنید سپس نتایج به دست آمده از هر پرسمان را به عنوان predicted results آن پرسمان در نظر بگیرید. همچنین در هر مورد لیستی از شناسه‌ها وجود دارد. این لیست را به عنوان actual results در نظر بگیرید.\n",
    "    <br>\n",
    "    <br>\n",
    "    معیار‌های زیر را پیاده‌سازی کنید (بدون استفاده از کد آماده) و نتیجه این معیارها را روی مجموعه‌ی actual و predicted گزارش کنید. دقت کنید که به ازای هر پرسمان و همچنین مجموع پرسمان‌ها باید تمام معیارها را در قالب مشخص شده گزارش کنید.\n",
    "    <br>\n",
    "    <br>\n",
    "    در این قسمت ۷ نمره به پیاده‌سازی صحیح و ۱۵ نمره به کیفیت نهایی بازیابی مربوط می‌شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def calculate_precision(actual: List[List[str]], predicted: List[List[str]], show=True) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the precision of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The precision of the predicted results\n",
    "    \"\"\"\n",
    "    precision = 0.0\n",
    "\n",
    "    # TODO: Calculate precision here\n",
    "    for i in range(len(actual)):\n",
    "        actual_ = actual[i]\n",
    "        pred = predicted[i]\n",
    "        \n",
    "        relevant = 0\n",
    "        for doc in pred:\n",
    "            if doc in actual_:\n",
    "                relevant += 1\n",
    "            \n",
    "        precision += np.round(relevant / len(pred), 3)\n",
    "        if show:\n",
    "            print(f'precision for {i}th query is :', np.round(relevant / len(pred), 3))\n",
    "\n",
    "    precision = precision / len(actual)\n",
    "\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(actual: List[List[str]], predicted: List[List[str]], show=True) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the recall of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The recall of the predicted results\n",
    "    \"\"\"\n",
    "    recall = 0.0\n",
    "\n",
    "    # TODO: Calculate recall here\n",
    "    for i in range(len(actual)):\n",
    "        actual_ = actual[i]\n",
    "        pred = predicted[i]\n",
    "        \n",
    "        relevant = 0\n",
    "        for doc in pred:\n",
    "            if doc in actual_:\n",
    "                relevant += 1\n",
    "    \n",
    "        recall += np.round(relevant / len(actual_), 3)\n",
    "        if show:\n",
    "            print(f'Recall for {i}th query is :', np.round(relevant / len(actual_), 3))\n",
    "        \n",
    "    recall = recall / len(actual)\n",
    "\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_F1(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the F1 score of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The F1 score of the predicted results    \n",
    "    \"\"\"\n",
    "    f1 = 0.0\n",
    "\n",
    "    # TODO: Calculate F1 here\n",
    "    p = calculate_precision(actual, predicted, show=False)\n",
    "    r = calculate_recall(actual, predicted, show=False)\n",
    "    \n",
    "    f1 = np.round((2 * p * r) / (p + r), 3)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAP(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Average Precision of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Mean Average Precision of the predicted results\n",
    "    \"\"\"\n",
    "    map = []\n",
    "    # TODO: Calculate MAP here\n",
    "    for query in range(len(actual)):\n",
    "        actual_ = actual[query]\n",
    "        pred = predicted[query]\n",
    "        \n",
    "        ap = 0\n",
    "        relevant = 0\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] in actual_:\n",
    "                relevant += 1\n",
    "                ap += relevant / (i+1)\n",
    "            \n",
    "        print(f'Average Precission for {query}th query: ', np.round(ap, 3))\n",
    "        \n",
    "        if relevant == 0:      # no relevant doc -> ap = 0\n",
    "            relevant = 1\n",
    "        map.append(np.round(ap/relevant, 3))\n",
    "        \n",
    "    map = np.round(np.mean(map), 3)\n",
    "    return map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cacluate_NDCG(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Normalized Discounted Cumulative Gain (NDCG) of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The NDCG of the predicted results\n",
    "    \"\"\"\n",
    "    # docs relevancy ranked from 10 to 1\n",
    "    \n",
    "    ndcg = []\n",
    "    prefect_ranking =[10] + [i/np.log2(11 - i) for i in range(9,0,-1)]\n",
    "    prefect_ranking = np.sum(prefect_ranking)\n",
    "    \n",
    "    # TODO: Calculate NDCG here\n",
    "    for query in range(len(actual)):\n",
    "        actual_ = actual[query]\n",
    "        pred = predicted[query]\n",
    "        \n",
    "        dcg = []\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] in actual_:\n",
    "                if i == 0:\n",
    "                    dcg.append(10 - actual_.index(pred[i]))\n",
    "                else:\n",
    "                    dcg.append((10 - actual_.index(pred[i])) / np.log2(i+1))\n",
    "                    \n",
    "        dcg = np.round(np.array(dcg) / prefect_ranking, 3)\n",
    "        sum_ = np.round(np.sum(dcg), 3)\n",
    "        print(f'NDCG for {query}th query: {dcg} = {sum_}')\n",
    "    \n",
    "        ndcg.append(sum_)\n",
    "        \n",
    "    ndcg = np.round(np.mean(ndcg), 3)\n",
    "    \n",
    "    return ndcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cacluate_MRR(actual: List[List[str]], predicted: List[List[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Mean Reciprocal Rank of the predicted results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : List[List[str]]\n",
    "        The actual results\n",
    "    predicted : List[List[str]]\n",
    "        The predicted results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The MRR of the predicted results\n",
    "    \"\"\"\n",
    "    MRR = []\n",
    "\n",
    "    # TODO: Calculate MRR here\n",
    "    for i in range(len(actual)):\n",
    "        actual_ = actual[i]\n",
    "        pred = predicted[i]\n",
    "        \n",
    "        RR = [1 / (doc_pos+1) for doc_pos in range(len(pred)) if pred[doc_pos] in actual_]\n",
    "        RR = np.round(np.mean(RR), 3) if RR != [] else 0\n",
    "        \n",
    "        print(f'RR for {i}th query: ', RR)\n",
    "        MRR.append(RR)\n",
    "        \n",
    "    MRR = np.round(np.mean(MRR), 3)\n",
    "\n",
    "    return MRR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open('validation.json')\n",
    "eval = json.load(file)\n",
    "eval = {correct_text(query): value for query, value in eval.items()}\n",
    "actual = list(eval.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for 0th query is : 0.583\n",
      "precision for 1th query is : 0.25\n",
      "precision for 2th query is : 0.75\n",
      "precision for 3th query is : 0.75\n",
      "precision for 4th query is : 0.333\n",
      "precision for 5th query is : 0.833\n",
      "ltn.lnn precision = 0.5831666666666667\n",
      "\n",
      "Recall for 0th query is : 0.7\n",
      "Recall for 1th query is : 0.3\n",
      "Recall for 2th query is : 0.9\n",
      "Recall for 3th query is : 0.9\n",
      "Recall for 4th query is : 0.4\n",
      "Recall for 5th query is : 1.0\n",
      "ltn.lnn recall = 0.6999999999999998\n",
      "\n",
      "ltn.lnn F1 = 0.636\n",
      "\n",
      "Average Precission for 0th query:  5.782\n",
      "Average Precission for 1th query:  1.012\n",
      "Average Precission for 2th query:  8.789\n",
      "Average Precission for 3th query:  9.0\n",
      "Average Precission for 4th query:  0.999\n",
      "Average Precission for 5th query:  9.452\n",
      "ltn.lnn MAP = 0.722\n",
      "\n",
      "NDCG for 0th query: [0.254 0.178 0.085 0.049 0.087 0.02  0.009] = 0.682\n",
      "NDCG for 1th query: [0.141 0.076 0.05 ] = 0.267\n",
      "NDCG for 2th query: [0.254 0.141 0.018 0.099 0.049 0.109 0.03  0.018 0.051] = 0.769\n",
      "NDCG for 3th query: [0.226 0.282 0.16  0.071 0.012 0.044 0.02  0.056 0.062] = 0.933\n",
      "NDCG for 4th query: [0.05  0.038 0.008 0.016] = 0.112\n",
      "NDCG for 5th query: [0.282 0.198 0.053 0.056 0.061 0.022 0.01  0.068 0.073 0.047] = 0.87\n",
      "ltn.lnn NDCG = 0.606\n",
      "\n",
      "RR for 0th query:  0.317\n",
      "RR for 1th query:  0.187\n",
      "RR for 2th query:  0.312\n",
      "RR for 3th query:  0.314\n",
      "RR for 4th query:  0.111\n",
      "RR for 5th query:  0.287\n",
      "ltn.lnn MRR = 0.255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Call evaluation functions here and report the results for ltn.lnn\n",
    "predicted = [search(query, query, 12, 'ltn-lnn', weight=0.5) for query in eval.keys()]\n",
    "\n",
    "print(f\"ltn.lnn precision = {calculate_precision(actual, predicted)}\\n\")\n",
    "print(f\"ltn.lnn recall = {calculate_recall(actual, predicted)}\\n\")\n",
    "print(f\"ltn.lnn F1 = {calculate_F1(actual, predicted)}\\n\")\n",
    "print(f\"ltn.lnn MAP = {calculate_MAP(actual, predicted)}\\n\")\n",
    "print(f\"ltn.lnn NDCG = {cacluate_NDCG(actual, predicted)}\\n\")\n",
    "print(f\"ltn.lnn MRR = {cacluate_MRR(actual, predicted)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for 0th query is : 0.8\n",
      "precision for 1th query is : 0.2\n",
      "precision for 2th query is : 0.9\n",
      "precision for 3th query is : 0.7\n",
      "precision for 4th query is : 0.3\n",
      "precision for 5th query is : 0.9\n",
      "ltc.lnc precision = 0.6333333333333332\n",
      "\n",
      "Recall for 0th query is : 0.8\n",
      "Recall for 1th query is : 0.2\n",
      "Recall for 2th query is : 0.9\n",
      "Recall for 3th query is : 0.7\n",
      "Recall for 4th query is : 0.3\n",
      "Recall for 5th query is : 0.9\n",
      "ltc.lnc recall = 0.6333333333333332\n",
      "\n",
      "ltc.lnc F1 = 0.633\n",
      "\n",
      "Average Precission for 0th query:  7.621\n",
      "Average Precission for 1th query:  0.583\n",
      "Average Precission for 2th query:  8.789\n",
      "Average Precission for 3th query:  5.607\n",
      "Average Precission for 4th query:  1.708\n",
      "Average Precission for 5th query:  9.0\n",
      "ltc.lnc MAP = 0.765\n",
      "\n",
      "NDCG for 0th query: [0.056 0.254 0.107 0.141 0.012 0.04  0.075 0.062] = 0.747\n",
      "NDCG for 1th query: [0.125 0.009] = 0.134\n",
      "NDCG for 2th query: [0.254 0.141 0.125 0.014 0.122 0.044 0.06  0.018 0.025] = 0.803\n",
      "NDCG for 3th query: [0.226 0.178 0.056 0.109 0.055 0.01  0.025] = 0.659\n",
      "NDCG for 4th query: [0.226 0.109 0.047] = 0.382\n",
      "NDCG for 5th query: [0.282 0.198 0.089 0.113 0.049 0.066 0.01  0.028 0.08 ] = 0.915\n",
      "ltc.lnc NDCG = 0.607\n",
      "\n",
      "RR for 0th query:  0.333\n",
      "RR for 1th query:  0.229\n",
      "RR for 2th query:  0.312\n",
      "RR for 3th query:  0.313\n",
      "RR for 4th query:  0.431\n",
      "RR for 5th query:  0.314\n",
      "ltc.lnc MRR = 0.322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Call evaluation functions here and report the results for ltc.lnc\n",
    "predicted = [search(query, query, 10, 'ltc-lnc', weight=0.5) for query in eval.keys()]\n",
    "\n",
    "print(f\"ltc.lnc precision = {calculate_precision(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc recall = {calculate_recall(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc F1 = {calculate_F1(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc MAP = {calculate_MAP(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc NDCG = {cacluate_NDCG(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc MRR = {cacluate_MRR(actual, predicted)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for 0th query is : 0.7\n",
      "precision for 1th query is : 0.2\n",
      "precision for 2th query is : 0.9\n",
      "precision for 3th query is : 0.8\n",
      "precision for 4th query is : 0.5\n",
      "precision for 5th query is : 0.9\n",
      "ltc.lnc precision = 0.6666666666666666\n",
      "\n",
      "Recall for 0th query is : 0.7\n",
      "Recall for 1th query is : 0.2\n",
      "Recall for 2th query is : 0.9\n",
      "Recall for 3th query is : 0.8\n",
      "Recall for 4th query is : 0.5\n",
      "Recall for 5th query is : 0.9\n",
      "ltc.lnc recall = 0.6666666666666666\n",
      "\n",
      "ltc.lnc F1 = 0.667\n",
      "\n",
      "Average Precission for 0th query:  6.732\n",
      "Average Precission for 1th query:  1.5\n",
      "Average Precission for 2th query:  8.789\n",
      "Average Precission for 3th query:  7.764\n",
      "Average Precission for 4th query:  2.4\n",
      "Average Precission for 5th query:  8.789\n",
      "ltc.lnc MAP = 0.853\n",
      "\n",
      "NDCG for 0th query: [0.254 0.169 0.178 0.056 0.024 0.08  0.009] = 0.77\n",
      "NDCG for 1th query: [0.141 0.099] = 0.24\n",
      "NDCG for 2th query: [0.254 0.198 0.089 0.014 0.049 0.109 0.02  0.027 0.051] = 0.811\n",
      "NDCG for 3th query: [0.254 0.226 0.178 0.071 0.012 0.044 0.019 0.062] = 0.866\n",
      "NDCG for 4th query: [0.226 0.061 0.022 0.038 0.051] = 0.398\n",
      "NDCG for 5th query: [0.282 0.141 0.125 0.056 0.036 0.011 0.08  0.018 0.051] = 0.8\n",
      "ltc.lnc NDCG = 0.648\n",
      "\n",
      "RR for 0th query:  0.364\n",
      "RR for 1th query:  0.625\n",
      "RR for 2th query:  0.312\n",
      "RR for 3th query:  0.336\n",
      "RR for 4th query:  0.218\n",
      "RR for 5th query:  0.312\n",
      "ltc.lnc MRR = 0.361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Call evaluation functions here and report the results for lnc.ltn\n",
    "predicted = [search(query, query, 10, 'lnc-ltn', weight=0.7) for query in eval.keys()]\n",
    "\n",
    "print(f\"ltc.lnc precision = {calculate_precision(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc recall = {calculate_recall(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc F1 = {calculate_F1(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc MAP = {calculate_MAP(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc NDCG = {cacluate_NDCG(actual, predicted)}\\n\")\n",
    "print(f\"ltc.lnc MRR = {cacluate_MRR(actual, predicted)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for 0th query is : 0.5\n",
      "precision for 1th query is : 0.3\n",
      "precision for 2th query is : 0.8\n",
      "precision for 3th query is : 0.8\n",
      "precision for 4th query is : 0.5\n",
      "precision for 5th query is : 0.8\n",
      "Okapi BM25 precision = 0.6166666666666667\n",
      "\n",
      "Recall for 0th query is : 0.5\n",
      "Recall for 1th query is : 0.3\n",
      "Recall for 2th query is : 0.8\n",
      "Recall for 3th query is : 0.8\n",
      "Recall for 4th query is : 0.5\n",
      "Recall for 5th query is : 0.8\n",
      "Okapi BM25 recall = 0.6166666666666667\n",
      "\n",
      "Okapi BM25 F1 = 0.617\n",
      "\n",
      "Average Precission for 0th query:  4.5\n",
      "Average Precission for 1th query:  1.233\n",
      "Average Precission for 2th query:  7.328\n",
      "Average Precission for 3th query:  7.764\n",
      "Average Precission for 4th query:  2.206\n",
      "Average Precission for 5th query:  7.435\n",
      "Okapi BM25 MAP = 0.761\n",
      "\n",
      "NDCG for 0th query: [0.254 0.113 0.142 0.028 0.051] = 0.588\n",
      "NDCG for 1th query: [0.089 0.085 0.022] = 0.196\n",
      "NDCG for 2th query: [0.254 0.056 0.018 0.056 0.061 0.075 0.062 0.025] = 0.607\n",
      "NDCG for 3th query: [0.254 0.113 0.089 0.028 0.097 0.109 0.066 0.009] = 0.765\n",
      "NDCG for 4th query: [0.071 0.097 0.022 0.009 0.053] = 0.252\n",
      "NDCG for 5th query: [0.282 0.141 0.125 0.014 0.097 0.04  0.053 0.025] = 0.777\n",
      "Okapi BM25 NDCG = 0.531\n",
      "\n",
      "RR for 0th query:  0.437\n",
      "RR for 1th query:  0.233\n",
      "RR for 2th query:  0.327\n",
      "RR for 3th query:  0.336\n",
      "RR for 4th query:  0.171\n",
      "RR for 5th query:  0.33\n",
      "Okapi BM25 MRR = 0.306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Call evaluation functions here and report the results for Okapi BM25\n",
    "predicted = [search(query, query, 10, 'okapi25', weight=0.5) for query in eval.keys()]\n",
    "\n",
    "print(f\"Okapi BM25 precision = {calculate_precision(actual, predicted)}\\n\")\n",
    "print(f\"Okapi BM25 recall = {calculate_recall(actual, predicted)}\\n\")\n",
    "print(f\"Okapi BM25 F1 = {calculate_F1(actual, predicted)}\\n\")\n",
    "print(f\"Okapi BM25 MAP = {calculate_MAP(actual, predicted)}\\n\")\n",
    "print(f\"Okapi BM25 NDCG = {cacluate_NDCG(actual, predicted)}\\n\")\n",
    "print(f\"Okapi BM25 MRR = {cacluate_MRR(actual, predicted)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
